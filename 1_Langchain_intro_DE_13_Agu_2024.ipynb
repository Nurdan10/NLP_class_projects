{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c26845d",
      "metadata": {
        "id": "2c26845d"
      },
      "source": [
        "# **LangChain'e Giriş: Yapay Zeka Uygulamalarını Kolaylaştırma**\n",
        "\n",
        "Bu Colab Notebooku, Harrison Chase tarafından Ekim 2022'de açık kaynaklı proje olarak başlatılan ve geliştirilen yeni bir yapay zeka (AI) çerçevesi olan LangChain'i keşfetmenize yardımcı olmak için tasarlanmıştır. LangChain, büyük dil modelleri (LLM'ler) kullanarak güçlü ve ilgi çekici AI uygulamaları oluşturmayı kolaylaştıran açık kaynaklı bir platformdur.\n",
        "\n",
        "Langchain Nisan 2023'de ise şirket olarak kurulmuştur.\n",
        "\n",
        "## **LangChain'in Faydaları:**\n",
        "\n",
        "**Kullanımı Kolay:** LangChain, karmaşık AI kodlama becerilerine sahip olmasanız bile LLM'leri kullanmanıza olanak tanıyan sezgisel bir API sağlar.\n",
        "\n",
        "\n",
        "**Esnek:** LangChain, çeşitli görevleri gerçekleştirmek üzere özelleştirilebilen modüler bir mimariye sahiptir.\n",
        "\n",
        "**Güçlü:** LangChain, OpenAI, Gemini, Claude ve diğer en son LLM'leri de dahil olmak üzere çeşitli güçlü modellerden yararlanır."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [**Langchain**](https://python.langchain.com/docs/get_started/quickstart)\n",
        "\n",
        "LangChain, AI uygulamalarının geliştirilmesini ve kullanılmasını kolaylaştırmak için tasarlanmış bir framework'dür. LangChain ile LLM'leri metin oluşturmak, dilleri tercüme etmek, kod yazmak ve çok daha fazlası için kullanabilirsiniz.\n",
        "\n",
        "## **LangChain consists of several components, including:**\n",
        "\n",
        "*   **Memory:** Used to store and manage data.\n",
        "*   **Chains:** Composed of a series of operations that perform a specific task.\n",
        "*   **Agents:** Used to run chains and interact with memory.\n",
        "*   **Tools:** Provide various utilities to make development easier.\n",
        "*   **Toolkits:** Easily use more than 1 tool together\n",
        "\n",
        "## **What You Can Do with LangChain:**\n",
        "\n",
        "LangChain can be used to perform a variety of tasks, such as:\n",
        "\n",
        "*   **Generating text**\n",
        "*   **Translating languages**\n",
        "*   **Writing code**\n",
        "*   **Answering questions**\n",
        "*   **Summarizing**\n",
        "*   **Classifying**"
      ],
      "metadata": {
        "id": "IG_Rohjs2Pp_"
      },
      "id": "IG_Rohjs2Pp_"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azZn36OAux4F",
        "outputId": "2724707b-ef6b-4e53-f5db-735371673c63"
      },
      "id": "azZn36OAux4F",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m739.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.8/384.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhnUV7dAUjVN",
        "outputId": "048b464d-b7df-4379-c33d-0ca73adbc3f1"
      },
      "id": "NhnUV7dAUjVN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8j1QppD4WIp",
        "outputId": "2ce2e029-e799-43b9-bb72-2d01af4d54ee"
      },
      "id": "J8j1QppD4WIp",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/361.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.3/361.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f31c4cc6",
      "metadata": {
        "id": "f31c4cc6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed0dc6a",
      "metadata": {
        "id": "9ed0dc6a"
      },
      "source": [
        "## LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fa352d5f",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa352d5f",
        "outputId": "c8970812-b14b-4dea-e491-ffc52788b52f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='\"German Treasures\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 28, 'total_tokens': 33}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None} id='run-469a4b64-f4a9-4a32-93ad-8fcf44de93b2-0' usage_metadata={'input_tokens': 28, 'output_tokens': 5, 'total_tokens': 33}\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 temperature=0.1,\n",
        "                 top_p=1.0)\n",
        "name = llm.invoke(\"I want to open a store selling regional materials from Germany. Suggest just a fantastic name for the store.\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(name.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EdZKNxtGF6p",
        "outputId": "910c5baa-f86c-4cdc-f870-1cd644044f74"
      },
      "id": "8EdZKNxtGF6p",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"German Treasures\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0782a2dd",
      "metadata": {
        "id": "0782a2dd"
      },
      "source": [
        "## Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7a306b9d",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a306b9d",
        "outputId": "8bbf78f4-1d81-46e6-8cbe-2596bc5231ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to open a store selling regional products from Germany. Suggest just a fantastic name for the store.\n"
          ]
        }
      ],
      "source": [
        "# langchain.prompts: Langchain, büyük dil modelleriyle (LLM'ler) çalışmayı kolaylaştırmak için tasarlanmış güçlü bir kütüphanedir.\n",
        "# langchain.prompts modülü, LLM'ler için promptları verimli bir şekilde tasarlamak için araçlar sunar.\n",
        "\n",
        "# Prompt Templates: LLM'lere vermek istediğiniz input için yeniden kullanılabilir bir yapı oluşturur. Effektif promptlar oluşturmak için\n",
        "# daha sonra doldurulan yer tutucuları (placeholders-input variables) içerir.\n",
        "\n",
        "# Input Variables: Bunlar, şablonunuzdaki prompt içeriğini dinamik olarak değiştirmenize olanak tanıyan yer tutuculardır(placeholders).\n",
        "# Formatting: Şablon içindeki yer tutucuların(input variables) gerçek değerlerle değiştirilmesi işlemidir.\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['country'],\n",
        "    template = \"I want to open a store selling regional products from {country}. Suggest just a fantastic name for the store.\"\n",
        ")\n",
        "p = prompt_template_name.format(country=\"Germany\")\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(p).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ImCvVPYF8OQ",
        "outputId": "12b83793-1e62-4638-ad56-ced71b51b98f"
      },
      "id": "-ImCvVPYF8OQ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"German Treasures\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for multiple inputs\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['product','country'],\n",
        "    template = \"I want to open a store selling regional {product} from {country}. Suggest just a fantastic name for this.\"\n",
        ")\n",
        "p = prompt_template_name.format(product=\"dessert\", country=\"Germany\")\n",
        "print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVFdTqUV70wE",
        "outputId": "d73ff0f8-54e4-4825-9112-9803a004320a"
      },
      "id": "DVFdTqUV70wE",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to open a store selling regional dessert from Germany. Suggest just a fantastic name for this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(p).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMFqIe4A7037",
        "outputId": "4f416a2b-0f70-437c-d25c-1906d496cf6a"
      },
      "id": "PMFqIe4A7037",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Sweet Heimat\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af406b92",
      "metadata": {
        "id": "af406b92"
      },
      "source": [
        "## Chains\n",
        "\n",
        "**LLM Chains:** Büyük Dil Modelleriyle Çalışmak İçin Güçlü Bir Tool'dür\n",
        "\n",
        "What are LLM Chains?\n",
        "\n",
        "LLM chains, büyük dil modelleriyle (LLM'ler) çalışmak üzere tasarlanmış özel Langchain bileşenleridir. LLM'leri promptlarla kolayca entegre etmenize ve zincirleme iş akışları oluşturmanıza olanak tanır.\n",
        "\n",
        "**Importance of LLM Chains:**\n",
        "\n",
        "* **LLM Yeteneklerini Genişletme:** LLM'ler metin oluşturma ve anlama açısından güçlü olsa da sınırlı yeteneklere sahiptir. LangChain Zincirleri, LLM'leri harici araçlarla (API'ler, calculator, databases vb.) birleştirerek bu sınırlamaların ötesine geçmenize olanak tanır.\n",
        "\n",
        "* **Veri işleme Esnekliği:** LLM Zincirleri, veri işleme için esnek frameworkler sağlar. LLM'ler zincirdeki diğer araçların (tools) çıktılarını kullanarak ham verileri anlayabilir ve soruları yanıtlayabilir.\n",
        "\n",
        "* **Karmaşık İşlevlerin Kolay Oluşturulması:** Zincirler, \"Araştırın, özetleyin ve ardından bu bilgiye dayanarak sorumu yanıtlayın\" gibi karmaşık talimatları step step işleyebileceği daha basit talimatlara dönüştürür.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "* **LLM Entegrasyonu:** LLM zincirleri, GPT-3 veya Jurassic-1 Jumbo gibi farklı LLM'lerini promptlarla entegre etmenize olanak tanır.\n",
        "* **İş Akışlarını Zincirleme:** LLM zincirleri, birden fazla LLM'i sırayla kullanarak karmaşık görevleri gerçekleştiren zincirleme iş akışları oluşturmanıza olanak tanır.\n",
        "* **Esneklik:** Farklı promptlar ve LLM'ler kullanarak LLM zincirlerini özelleştirebilir ve çeşitli alanlarda benzer zincirleme iş akışları oluşturabilirsiniz..\n",
        "\n",
        "**Advantages of LLM Chains:**\n",
        "\n",
        "* **Artırılmış Kapsamlılık:** Birden fazla LLM kullanmak, daha kapsamlı ve bilgilendirici sonuçlar elde etmenize olanak tanır.\n",
        "* **Artırılmış Verimlilik:** Tekrarlanan görevlerin otomatikleştirilmesi, zamandan ve emekten tasarruf sağlar.\n",
        "* **Artırılmış Esneklik:** LLM zincirleri, farklı promptlar ve LLM'ler kullanılarak özelleştirilebilir ve çeşitli görevlere uyarlanabilir."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxYu7FA3s9kk",
        "outputId": "09375dfd-21ac-4662-dbb4-7de5d1079e06"
      },
      "id": "VxYu7FA3s9kk",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['country', 'product'], template='I want to open a store selling regional {product} from {country}. Suggest just a fantastic name for this.')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ba65c213",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba65c213",
        "outputId": "913c3d31-5f6a-465e-97d2-0c5c91fbc0fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='\"Sweet Anatolia\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 27, 'total_tokens': 32}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_507c9469a1', 'finish_reason': 'stop', 'logprobs': None}, id='run-2812e1a6-93e9-4bf1-a9c9-3b831041c609-0', usage_metadata={'input_tokens': 27, 'output_tokens': 5, 'total_tokens': 32})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "chain = prompt_template_name | llm #LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "name= chain.invoke({\"product\":\"dessert\", \"country\":\"Turkey\"})\n",
        "name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(name.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8c_pSA5HYB9",
        "outputId": "3b04a331-95c7-4e99-8564-f0685d0d7bc6"
      },
      "id": "_8c_pSA5HYB9",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Sweet Anatolia\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "\n",
        "chain = prompt_template_name | llm | StrOutputParser() #LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "name= chain.invoke({\"product\":\"dessert\", \"country\":\"Turkey\"})\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoK2Oyojcwj5",
        "outputId": "a806aa13-f1f5-4909-a6cf-a0d44d78cf30"
      },
      "id": "GoK2Oyojcwj5",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Sweet Anatolia\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e5ccee75",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5ccee75",
        "outputId": "9814cae5-3175-40bd-d2f1-0019d7032b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'product': 'dessert', 'country': 'Turkey', 'text': '\"Sweet Anatolia\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "chain.invoke({\"product\":\"dessert\", \"country\":\"Turkey\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sequential Chain"
      ],
      "metadata": {
        "id": "8U3BnOnDDCXM"
      },
      "id": "8U3BnOnDDCXM"
    },
    {
      "cell_type": "code",
      "source": [
        "llm_1 = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                   temperature=0.6,\n",
        "                   top_p=1.0)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['product','country'],\n",
        "    template = \"I want to open a store selling regional {product} from {country}. Suggest just a fantastic name for this.\"\n",
        ")\n",
        "\n",
        "name_chain = prompt_template_name | llm_1 | StrOutputParser()\n",
        "\n",
        "llm_2 = ChatOpenAI(model=\"gpt-4o-2024-08-06\",\n",
        "                  temperature=0.4,\n",
        "                  top_p=1.0)\n",
        "\n",
        "prompt_template_products = PromptTemplate(\n",
        "    input_variables = ['store_name'],\n",
        "    template=\"recommend just some products that I can sell in the store based on {store_name}\"\n",
        ")\n",
        "\n",
        "products_chain = {\"store_name\": name_chain} | prompt_template_products | llm_2 | StrOutputParser()\n",
        "\n",
        "content=products_chain.invoke({\"product\":\"dessert\", \"country\":\"Spain\"})\n",
        "\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O7hKEtbdk6l",
        "outputId": "be833bc6-2fba-4374-d101-32e9e32f135a"
      },
      "id": "1O7hKEtbdk6l",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Sweet España Delights\" suggests a focus on Spanish sweets and delicacies. Here are some product recommendations that would fit well in such a store:\n",
            "\n",
            "1. **Turrón**: A traditional Spanish nougat made from almonds, honey, sugar, and egg whites. Available in varieties like Alicante (hard) and Jijona (soft).\n",
            "\n",
            "2. **Churros Mix**: Offer a mix that customers can use to make churros at home, along with a chocolate dipping sauce.\n",
            "\n",
            "3. **Polvorones and Mantecados**: These are traditional Spanish shortbread cookies, often enjoyed during the holiday season.\n",
            "\n",
            "4. **Marzipan**: A sweet confection made from almond paste, sugar, and egg whites, often shaped into fruits or other figures.\n",
            "\n",
            "5. **Yemas de Santa Teresa**: A sweet made from egg yolks and sugar, originating from Ávila, Spain.\n",
            "\n",
            "6. **Rosquillas**: Spanish doughnuts that come in various flavors and styles, such as anise or lemon.\n",
            "\n",
            "7. **Pestiños**: A traditional Andalusian pastry, often flavored with honey or sugar and sometimes anise.\n",
            "\n",
            "8. **Alfajores de Medina Sidonia**: A sweet treat made with almonds, honey, and spices, wrapped in a wafer.\n",
            "\n",
            "9. **Ensaimadas**: A sweet, spiral-shaped pastry from Mallorca, often filled with cream, chocolate, or pumpkin.\n",
            "\n",
            "10. **Caramelized Almonds**: A popular snack in Spain, these almonds are coated in a sweet, crunchy caramel shell.\n",
            "\n",
            "11. **Spanish Hot Chocolate**: A thick and rich chocolate drink mix, perfect for pairing with churros.\n",
            "\n",
            "12. **Fig Cake (Pan de Higo)**: A dense cake made from dried figs and nuts, often enjoyed with cheese.\n",
            "\n",
            "13. **Membrillo (Quince Paste)**: A sweet, dense jelly made from quince, typically served with cheese.\n",
            "\n",
            "14. **Horchata**: A refreshing drink made from tiger nuts, sugar, and water, popular in Valencia.\n",
            "\n",
            "15. **Spanish Saffron**: While not a sweet, saffron is a prized Spanish spice that can be used in various sweet and savory dishes.\n",
            "\n",
            "These products can provide a diverse and authentic selection of Spanish sweets and treats that will appeal to customers looking for traditional and unique flavors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "ykteVFRdBmFs",
        "outputId": "481dbcb9-83fb-41dc-ba97-0fb1e033bf1f"
      },
      "id": "ykteVFRdBmFs",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\"Sweet España Delights\" suggests a focus on Spanish sweets and delicacies. Here are some product recommendations that would fit well in such a store:\n\n1. **Turrón**: A traditional Spanish nougat made from almonds, honey, sugar, and egg whites. Available in varieties like Alicante (hard) and Jijona (soft).\n\n2. **Churros Mix**: Offer a mix that customers can use to make churros at home, along with a chocolate dipping sauce.\n\n3. **Polvorones and Mantecados**: These are traditional Spanish shortbread cookies, often enjoyed during the holiday season.\n\n4. **Marzipan**: A sweet confection made from almond paste, sugar, and egg whites, often shaped into fruits or other figures.\n\n5. **Yemas de Santa Teresa**: A sweet made from egg yolks and sugar, originating from Ávila, Spain.\n\n6. **Rosquillas**: Spanish doughnuts that come in various flavors and styles, such as anise or lemon.\n\n7. **Pestiños**: A traditional Andalusian pastry, often flavored with honey or sugar and sometimes anise.\n\n8. **Alfajores de Medina Sidonia**: A sweet treat made with almonds, honey, and spices, wrapped in a wafer.\n\n9. **Ensaimadas**: A sweet, spiral-shaped pastry from Mallorca, often filled with cream, chocolate, or pumpkin.\n\n10. **Caramelized Almonds**: A popular snack in Spain, these almonds are coated in a sweet, crunchy caramel shell.\n\n11. **Spanish Hot Chocolate**: A thick and rich chocolate drink mix, perfect for pairing with churros.\n\n12. **Fig Cake (Pan de Higo)**: A dense cake made from dried figs and nuts, often enjoyed with cheese.\n\n13. **Membrillo (Quince Paste)**: A sweet, dense jelly made from quince, typically served with cheese.\n\n14. **Horchata**: A refreshing drink made from tiger nuts, sugar, and water, popular in Valencia.\n\n15. **Spanish Saffron**: While not a sweet, saffron is a prized Spanish spice that can be used in various sweet and savory dishes.\n\nThese products can provide a diverse and authentic selection of Spanish sweets and treats that will appeal to customers looking for traditional and unique flavors."
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Tools, Toolkits and Agents: Büyük Dil Modellerini Güçlendirmek\n",
        "\n",
        "Langchain, GenerativeAI modellerini geliştirmek için tasarlanmış bir toolkit ve agent  framework'üdür. Bunu, modellere ek işlevler sağlayan ve onları daha güçlü hale getiren tool ve agent'lar aracılığıyla yapar.\n",
        "\n",
        "**LangChain Tools**\n",
        "\n",
        "Bunları bir LLM'in süper güçleri olarak düşünün. Tıpkı bir süper kahramanın tool (web, hesap makinesi vb.)'leri kullanması gibi, bir dil modeli de (GPT-4 gibi) özel şeyler yapmak için LangChain tool'lerini kullanabilir.\n",
        "\n",
        "**Example Tools:**\n",
        "\n",
        "* **Search Engine:** LLM bilgi için internette arama yapmasına olanak tanır.\n",
        "* **Calculator:** LLM'in daha iyi hesaplamalar yapmasını sağlar.\n",
        "* **Wikipedia:** LLM'in wikipedia'daki bilgileri baz olarak daha doğru cevaplar vermesini sağlar.\n",
        "\n",
        "**Langchain Toolkits**\n",
        "\n",
        "toolkit, belirli bir görev yelpazesini ele almak için tasarlanmış bir dizi Tool'dür. Süper Kahramanın aynı anda birden fazla süper gücünü kullanması gibi düşünebilirsiniz.\n",
        "\n",
        "Here are some examples of toolkits:\n",
        "\n",
        "* **Natural Language Processing Toolkit:** Bu araç seti, metin özetleme, duygu analizi ve metin oluşturma gibi görevlere yönelik araçlar içerir.\n",
        "* **Machine Learning Toolkit:** Bu araç seti, veri ön işleme, model eğitimi ve model değerlendirme gibi görevlere yönelik araçlar içerir.\n",
        "* **Data Science Toolkit:** Bu araç seti, veri görselleştirme, veri temizleme ve istatistiksel analiz gibi görevlere yönelik araçlar içerir.\n",
        "\n",
        "**LangChain Agents**\n",
        "\n",
        "* Onları operasyonun arkasındaki beyinler olarak düşünün. Bir agent, LLM için bir görev planlayıcı gibidir. Hangi araçların ne zaman kullanılacağına karar verir.\n",
        "* **The mission:** Agent görevi yerine getirmek için LLM'lerin süper güçlerini (tools) kullanır.\n",
        "\n",
        "**Analogy: The Superhero Team**\n",
        "\n",
        "* **The Language Model:** Bu, dili anlama ve üretme konusunda temel yeteneğe sahip olan süper kahramanınızdır.\n",
        "* **The Tools:** Bunlar süper kahramanınızın süper güçleridir: internete bağlanma, wikipedia'ya bağlanma, hesaplama desteği alma.\n",
        "* **The Toolkits:** Bunu, birden fazla süper gücü aynı anda kullanan süper kahraman olarak tanımlayabiliriz.\n",
        "* **The Agent:** Bunu, Yenilmezler(The Avengers)'deki Nick Fury gibi bir strateji uzmanı olarak düşünebiliriz. Ajan, süper kahramanlara görevler verir ve onlara güçlerini ne zaman kullanacaklarını söyler.\n",
        "\n",
        "\n",
        "**How It Works Together**\n",
        "\n",
        "* **The Task:** Agent, \"Fransa ekonomisi hakkında bir rapor yazın\" gibi bir görev alır.\n",
        "* **The Plan:** Agent karar verir:\n",
        "Güncel bilgilere ihtiyacım var (Arama Motoru aracı)\n",
        "Analiz edilecek karmaşık veriler olabilir (Calculator aracı)\n",
        "* **The Action:** Agent, LLM'e şunları söyler:\n",
        "\"İnternette 'Fransa ekonomisi' diye arama yapın.\"\n",
        "\"Önemli noktaları özetleyin ve veriler üzerinde bazı hesaplamalar yapın.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "OdOCRlO9VEmH"
      },
      "id": "OdOCRlO9VEmH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### [Langchain Tools](https://python.langchain.com/docs/modules/agents/tools/)"
      ],
      "metadata": {
        "id": "C7aXK0NZ2E5Q"
      },
      "id": "C7aXK0NZ2E5Q"
    },
    {
      "cell_type": "markdown",
      "id": "471b2c6b",
      "metadata": {
        "id": "471b2c6b"
      },
      "source": [
        "### [Tavily Search](https://python.langchain.com/v0.1/docs/integrations/tools/tavily_search/)\n",
        "\n",
        "##### [Please get apikey for internet search](https://tavily.com/)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "iKhD5yq-zk3s"
      },
      "id": "iKhD5yq-zk3s",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "search_tool = TavilySearchResults(max_results=1)"
      ],
      "metadata": {
        "id": "NdBj8v7DlWgJ"
      },
      "id": "NdBj8v7DlWgJ",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"Obama's first name?\"\n",
        "output=search_tool.invoke(question)\n",
        "output\n",
        "\n",
        "# The output returned after the imputation we gave to the Google search engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD3ZDDPCT_GW",
        "outputId": "216a2e26-d77a-4364-90ab-00d2cfa12e07"
      },
      "id": "SD3ZDDPCT_GW",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://en.wikipedia.org/wiki/Early_life_and_career_of_Barack_Obama',\n",
              "  'content': \"He served on the board of directors of the Woods Fund of Chicago, which in 1985 had been the first foundation to fund Obama's DCP, from 1993 to 2002, and served on the board of directors of The Joyce Foundation from 1994 to 2002.[55] Membership on the Joyce and Wood foundation boards, which gave out tens of millions of dollars to various local organizations while Obama was a member, helped Obama get to know and be known by influential liberal groups and cultivate a network of community activists that later supported his political career.[69] Obama served on the board of directors of the Chicago Annenberg Challenge from 1995 to 2002, as founding president and chairman of the board of directors from 1995 to 1999.[55] They married on the Hawaiian island of Maui on February 2, 1961.[6]\\nBarack Hussein Obama II, born in Honolulu on August 4, 1961, at the old Kapiolani Maternity and Gynecological Hospital at 1611 Bingham Street (a predecessor of the Kapiʻolani Medical Center for Women and Children at 1319 Punahou Street), was named for his father.[4][7][8]\\nThe Honolulu Advertiser and the Honolulu Star-Bulletin announced the birth.[9]\\nSoon after their son's birth, while Obama's father continued his education at the University of Hawaii, Ann Dunham took the infant to Seattle, Washington, where she took classes at the University of Washington from September 1961 to June 1962. Two of these cases involved ACORN suing Governor Jim Edgar under the new Motor Voter Act,[78][79] one involved a voter suing Mayor Daley under the Voting Rights Act,[80] and one involved, in the only case Obama orally argued, a whistleblowing stockbroker suing his former employer.[81]\\nAll of these appeals were resolved in favor of Obama's clients, with all the opinions authored by Obama's University of Chicago colleague Chief Judge Richard Posner.[82]\\nObama was a founding member of the board of directors of Public Allies in 1992, resigning before his wife, Michelle, became the founding executive director of Public Allies Chicago in early 1993.[55][83] From sixth grade through eighth grade at Punahou, Obama lived with his mother and Maya.[35][36]\\nObama's mother completed her coursework at the University of Hawaii for an M.A. in anthropology in December 1974.[37] After three years in Hawaii, she and Maya returned to Jakarta in August 1975,[38] where Dunham completed her contract with the Institute of Management Education and Development and started anthropological fieldwork.[39]\\nObama chose to stay with his grandparents in Honolulu to continue his studies at Punahou School for his high school years.[8][40]\\n In the summer of 1981, Obama traveled to Jakarta to visit his mother and half-sister Maya, and visited the families of Occidental College friends in Hyderabad (India) and Karachi (Pakistan) for three weeks.[49]\\nHe then transferred to Columbia University in New York City, where he majored in political science with a speciality in international relations[50][51] and in English literature.[52] Obama lived off campus in a modest rented apartment at 142 West 109th Street.[53][54]\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['question','output'],\n",
        "    template = \"Answer the question of '{question}' according to '{output}'\"\n",
        ")\n",
        "p = prompt_template_name.format(question=question, output=output)\n",
        "print(p)\n",
        "\n",
        "# We are creating a prompt template to use in the llmchain I will create below."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MlkI8OWZqyE",
        "outputId": "518a6427-1d1e-40ae-8ded-b030fbb18bf3"
      },
      "id": "4MlkI8OWZqyE",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question of 'Obama's first name?' according to '[{'url': 'https://en.wikipedia.org/wiki/Early_life_and_career_of_Barack_Obama', 'content': \"He served on the board of directors of the Woods Fund of Chicago, which in 1985 had been the first foundation to fund Obama's DCP, from 1993 to 2002, and served on the board of directors of The Joyce Foundation from 1994 to 2002.[55] Membership on the Joyce and Wood foundation boards, which gave out tens of millions of dollars to various local organizations while Obama was a member, helped Obama get to know and be known by influential liberal groups and cultivate a network of community activists that later supported his political career.[69] Obama served on the board of directors of the Chicago Annenberg Challenge from 1995 to 2002, as founding president and chairman of the board of directors from 1995 to 1999.[55] They married on the Hawaiian island of Maui on February 2, 1961.[6]\\nBarack Hussein Obama II, born in Honolulu on August 4, 1961, at the old Kapiolani Maternity and Gynecological Hospital at 1611 Bingham Street (a predecessor of the Kapiʻolani Medical Center for Women and Children at 1319 Punahou Street), was named for his father.[4][7][8]\\nThe Honolulu Advertiser and the Honolulu Star-Bulletin announced the birth.[9]\\nSoon after their son's birth, while Obama's father continued his education at the University of Hawaii, Ann Dunham took the infant to Seattle, Washington, where she took classes at the University of Washington from September 1961 to June 1962. Two of these cases involved ACORN suing Governor Jim Edgar under the new Motor Voter Act,[78][79] one involved a voter suing Mayor Daley under the Voting Rights Act,[80] and one involved, in the only case Obama orally argued, a whistleblowing stockbroker suing his former employer.[81]\\nAll of these appeals were resolved in favor of Obama's clients, with all the opinions authored by Obama's University of Chicago colleague Chief Judge Richard Posner.[82]\\nObama was a founding member of the board of directors of Public Allies in 1992, resigning before his wife, Michelle, became the founding executive director of Public Allies Chicago in early 1993.[55][83] From sixth grade through eighth grade at Punahou, Obama lived with his mother and Maya.[35][36]\\nObama's mother completed her coursework at the University of Hawaii for an M.A. in anthropology in December 1974.[37] After three years in Hawaii, she and Maya returned to Jakarta in August 1975,[38] where Dunham completed her contract with the Institute of Management Education and Development and started anthropological fieldwork.[39]\\nObama chose to stay with his grandparents in Honolulu to continue his studies at Punahou School for his high school years.[8][40]\\n In the summer of 1981, Obama traveled to Jakarta to visit his mother and half-sister Maya, and visited the families of Occidental College friends in Hyderabad (India) and Karachi (Pakistan) for three weeks.[49]\\nHe then transferred to Columbia University in New York City, where he majored in political science with a speciality in international relations[50][51] and in English literature.[52] Obama lived off campus in a modest rented apartment at 142 West 109th Street.[53][54]\"}]'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = prompt_template_name | llm | StrOutputParser() #LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "name= chain.invoke({\"question\":question, \"output\":output})\n",
        "name\n",
        "\n",
        "# We get the answer to our question by combining the google search tool and LLM we created with the LLMchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AGi7C9JBAFa3",
        "outputId": "8bc4e22b-1130-4cf3-c8cb-8e0e48873857"
      },
      "id": "AGi7C9JBAFa3",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Obama's first name is Barack.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 temperature=0,\n",
        "                 top_p=1,\n",
        "                 max_tokens=250)\n",
        "\n",
        "ai_msg = llm.invoke(\"2023 yılı UEFA şampiyonlar ligini hangi futbol takımı kazanmıştır?\")\n",
        "ai_msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwUMXsARmBuj",
        "outputId": "51abfda3-e649-42f7-b81d-6f687ce0106a"
      },
      "id": "gwUMXsARmBuj",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Üzgünüm, ancak 2023 yılı UEFA Şampiyonlar Ligi'nin kazananı hakkında güncel bir bilgiye sahip değilim. En son bilgilerim Ekim 2021'e kadar güncellenmiştir. 2023 yılına ait sonuçları öğrenmek için güncel spor haberlerini kontrol etmenizi öneririm.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 24, 'total_tokens': 94}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-7944afbd-d0a4-418b-8291-9ea6de6bba28-0', usage_metadata={'input_tokens': 24, 'output_tokens': 70, 'total_tokens': 94})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"2023 şampiyonlar ligini hangi takım kazandı?\"\n",
        "output=search_tool.invoke(question)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWPCZlzTATo-",
        "outputId": "f64d125b-32c4-4701-b685-4c33e4f3e2c5"
      },
      "id": "GWPCZlzTATo-",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://tr.wikipedia.org/wiki/2023_UEFA_Şampiyonlar_Ligi_Finali',\n",
              "  'content': \"2023 UEFA Şampiyonlar Ligi finali, ... Maçı 68. dakikada Rodri'nin attığı golle Manchester City 1-0 kazandı ve 2023 UEFA Şampiyonlar Ligi şampiyonu oldu. [2] Takımlar. Takım Önceki oynadığı finaller (kalın:kazandığı yıl) Manchester City: 1 Internazionale: 5 (1964, 1965, 1967, 1972, 2010) Finale giden yol. Not: Skor ...\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template_name | llm | StrOutputParser()\n",
        "chain.invoke({\"question\":\"2023 şampiyonlar ligini hangi takım kazandı?\", \"output\":output})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "L7rqtZvicdlF",
        "outputId": "3d224290-58f2-4e9c-e63d-327144ad7867"
      },
      "id": "L7rqtZvicdlF",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"2023 UEFA Şampiyonlar Ligi'ni Manchester City kazandı. Finalde, 68. dakikada Rodri'nin attığı golle maçı 1-0'lık skorla galip tamamladılar.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Agent Types](https://python.langchain.com/docs/modules/agents/agent_types/)\n",
        "\n",
        "Evet, yukarıda gördüğünüz gibi öncelikle araçları(tools) oluşturmamız ve daha sonra bu araçların sonuçlarını LMM'ler ile birleştirmemiz gerekiyor. Bu işlemleri daha pratik bir şekilde gerçekleştirmek için Langchain Agent'ları kullanacağız.\n",
        "\n",
        "Seçeceğimiz Agent tipine göre kullanacağımız prompt değişecektir. Bu Agent'lar için hazırlanan default promptları langchain hub'ından çekeceğiz."
      ],
      "metadata": {
        "id": "73zf4tKqm4Ch"
      },
      "id": "73zf4tKqm4Ch"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is LangChain Hub?\n",
        "**LangChain Hub**, LangChain ile çalışan developerların prompts, chains(zincirler) ve agents gibi LLM bileşenlerini paylaşabileceği, keşfedebileceği ve yönetebileceği bir platformdur. Hugging Face Hub'dan ilham alan platform, LLM'leri kullanarak karmaşık LLM uygulamalarını geliştirmeyi kolaylaştırmayı amaçlıyor.\n",
        "\n",
        "**Features of LangChain Hub:**\n",
        "\n",
        "* **Sharing and Discovering Prompts:** Kullanıcılar kendi promptlarını platforma yükleyebilir ve diğer kullanıcılar tarafından oluşturulan promptlara göz atabilir..\n",
        "* **Chain Creation:** Karmaşık LLM uygulamaları oluşturmak amacıyla farklı prompt ve agentları birleştirmek için zincirler oluşturabilirsiniz.\n",
        "* **Version Management:** Promptların ve zincirlerin farklı sürümlerini takip edebilir ve uygulamanızda belirli bir sürümü kullanabilirsiniz.\n",
        "* **Easy Integration:** LangChain Hub, LangSmith ile entegredir; bu, promptları ve zincirleri doğrudan LangSmith arayüzünden kullanabileceğiniz anlamına gelir.\n",
        "\n",
        "**Benefits of LangChain Hub:**\n",
        "\n",
        "* **Rapid Development:** Platforma yüklenen hazır promptlar ve zincirler, LLM uygulamalarını geliştirme sürecini hızlandırır.\n",
        "*   **Innovative Applications:** Farklı LLM bileşenlerini birleştirerek yeni ve yaratıcı LLM uygulamaları oluşturabilirsiniz.\n",
        "*   **Community Engagement:** LangChain Hub, LLM developerlarının bir araya gelip bilgi paylaşması için bir platform sağlar.\n",
        "\n",
        "**[Langchainhub](https://smith.langchain.com/hub?organizationId=69f3fb0f-fa77-5041-8d83-c0240ae7e012)**"
      ],
      "metadata": {
        "id": "N8wTKy4OXIqE"
      },
      "id": "N8wTKy4OXIqE"
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN\"] = userdata.get('LANGCHAIN')"
      ],
      "metadata": {
        "id": "nZDSDFbJYpRz"
      },
      "id": "nZDSDFbJYpRz",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchainhub"
      ],
      "metadata": {
        "id": "CP26TER_bg6P"
      },
      "id": "CP26TER_bg6P",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We pull a prompt prepared for openai-tools-agent from the langchain hub. We can change this prompt if we want.\n",
        "\n",
        "from langchain import hub\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\", api_key=os.environ[\"LANGCHAIN\"])\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuXCcD8xmS8j",
        "outputId": "58223fa3-a6cb-466a-8535-78cc8997077b"
      },
      "id": "HuXCcD8xmS8j",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.messages[0].prompt.template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bLYjRROTU4rZ",
        "outputId": "0b6da7cc-06e1-46e3-ebf0-7b64cb671793"
      },
      "id": "bLYjRROTU4rZ",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are a helpful assistant'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt[0].prompt.template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3aRpnRVtdBTL",
        "outputId": "81bfda15-727c-4138-befe-9fd342745a1c"
      },
      "id": "3aRpnRVtdBTL",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are a helpful assistant'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change Default Prompt"
      ],
      "metadata": {
        "id": "4LvID5dhqPd8"
      },
      "id": "4LvID5dhqPd8"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt[0].prompt.template=\"\"\"Answer the question asked correctly. If you're not sure, answer \"I don't know.\" \"\"\""
      ],
      "metadata": {
        "id": "yYWSE_wUqVCP"
      },
      "id": "yYWSE_wUqVCP",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvzseKvVqVJI",
        "outputId": "58a810f7-a05d-4a19-d347-eb7f1f86333d"
      },
      "id": "gvzseKvVqVJI",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Answer the question asked correctly. If you\\'re not sure, answer \"I don\\'t know.\" ')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt[0].prompt.template=\"You are a helpful assistant\"\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN6ll26C6-Ux",
        "outputId": "c58f5dd4-122d-47a6-a97d-898444e86f62"
      },
      "id": "hN6ll26C6-Ux",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tavily-Search with Agent"
      ],
      "metadata": {
        "id": "9TVcmvcmfU05"
      },
      "id": "9TVcmvcmfU05"
    },
    {
      "cell_type": "code",
      "source": [
        "user_input=\"Which team won the 2023 UEFA champions league?\""
      ],
      "metadata": {
        "id": "DYc82jlUoG-d"
      },
      "id": "DYc82jlUoG-d",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate.from_template(\n",
        "    \"\"\"Answer correctly the question of \"{question}\". If you're not sure, answer \"I don't know.\" \"\"\" #input_variables =['question'],\n",
        ")\n",
        "prompt_template = prompt_template_name.format(question=user_input)\n",
        "prompt_template\n",
        "\n",
        "# prompt_template_name = PromptTemplate(\n",
        "#    input_variables =['question'],\n",
        "#    template =  \"\"\"Answer correctly the question of \"{question}\". If you're not sure, answer \"I don't know.\" \"\"\")\n",
        "# p = prompt_template_name.format(question=user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1dcsz9T2nToT",
        "outputId": "c0feacb5-a84a-4275-da60-c3c877b0e8b7"
      },
      "id": "1dcsz9T2nToT",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer correctly the question of \"Which team won the 2023 UEFA champions league?\". If you\\'re not sure, answer \"I don\\'t know.\" '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "                 model=\"gpt-4o-mini\",\n",
        "                 temperature=0,\n",
        "                 top_p=1)\n",
        "llm.invoke(prompt_template).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bbZ-GHCmhK_W",
        "outputId": "dd15a2d7-fac0-4413-e328-d6768a1b1b58"
      },
      "id": "bbZ-GHCmhK_W",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [OpenAI Tools Agent](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/openai_tools/)"
      ],
      "metadata": {
        "id": "zPhIyoWRFruv"
      },
      "id": "zPhIyoWRFruv"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\", api_key=os.environ[\"LANGCHAIN\"])\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN9J1wK5MJ0T",
        "outputId": "276f4625-badc-4365-df36-0d8d6dfe3253"
      },
      "id": "QN9J1wK5MJ0T",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "llm = ChatOpenAI(temperature=0.0,\n",
        "                 model=\"gpt-4o-mini\",\n",
        "                 top_p=1.0)\n",
        "\n",
        "# The tools we'll give the Agent access to.\n",
        "tools=[search_tool]\n",
        "\n",
        "# Finally, let's create an openai agent with the tools, the language model, and the prompt\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "# agent_executor performs all the preparations necessary to execute a query.\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# Let's test it out!\n",
        "agent_executor.invoke({\"input\": user_input})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCDrIgyMF4fs",
        "outputId": "a6afcb6f-83ad-4b91-e562-63de8863e54f"
      },
      "id": "oCDrIgyMF4fs",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `tavily_search_results_json` with `{'query': '2023 UEFA Champions League winner'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.sportingnews.com/us/soccer/news/who-won-champions-league-final-2023-man-city-inter-treble-rodri/ypj0yxkofcr9ki7zfnsi7i6c', 'content': \"MORE: Enjoy a minute-by-minute recap of the 2023 Champions League final as City win the European title\\nMan City win 2023 Champions League final\\nPep Guardiola and Manchester City have finally summited the mountain they began climbing seven years ago, winning the club's first European crown by topping Inter Milan 1-0 in the 2023 UEFA Champions League final.\\n The former Manchester United star had a cruel own goal in the 2020 Europa League final with Inter in defeat to Sevilla, and then suffered an outrageous string of misses with Belgium in the 2022 World Cup at the end of their eventual defeat to Croatia in the group stage.\\n Rodri gets breakthrough goal for Man City\\nFinally, the moment the game was begging for came in the 68th minute as Rodri smashed home the opening goal of the match, putting Man City in front.\\n A goal from Rodri in the 68th minute was enough to do the job, as Inter put up a true challenge but failed to find the back of the net through the match.\\n With nobody in the area, Rodri pounced, charging onto the loose ball and blasting a shot that curled around two Inter defenders and into the back of the net, as Andre Onana was rooted to the spot.\\n\"}]\u001b[0m\u001b[32;1m\u001b[1;3mManchester City won the 2023 UEFA Champions League, defeating Inter Milan 1-0 in the final. The only goal of the match was scored by Rodri in the 68th minute. This victory marked Manchester City's first European crown.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Which team won the 2023 UEFA champions league?',\n",
              " 'output': \"Manchester City won the 2023 UEFA Champions League, defeating Inter Milan 1-0 in the final. The only goal of the match was scored by Rodri in the 68th minute. This victory marked Manchester City's first European crown.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_tool.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2DOGkMBV-NkO",
        "outputId": "272edc4a-140a-41a2-a521-c57b5a66f13e"
      },
      "id": "2DOGkMBV-NkO",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tavily_search_results_json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Tool Calling Agent](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/)\n",
        "\n",
        "#### With Gemini Models"
      ],
      "metadata": {
        "id": "a-WxMszbMMNG"
      },
      "id": "a-WxMszbMMNG"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-genai # for gemini models"
      ],
      "metadata": {
        "id": "SSgh_VUBMtzY"
      },
      "execution_count": 45,
      "outputs": [],
      "id": "SSgh_VUBMtzY"
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "RRlvnObj9nL9"
      },
      "id": "RRlvnObj9nL9",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")"
      ],
      "metadata": {
        "id": "0rMJftp-Mtze"
      },
      "execution_count": 47,
      "outputs": [],
      "id": "0rMJftp-Mtze"
    },
    {
      "cell_type": "code",
      "source": [
        "search_tool.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IAyLwPra9sqD",
        "outputId": "d1ab5114-16e4-45b9-ff43-d08f4a8bb133"
      },
      "id": "IAyLwPra9sqD",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tavily_search_results_json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Make sure to use the tavily_search_results_json tool for information.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "tools=[search_tool]\n",
        "\n",
        "# Construct the Tools agent\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "agent_executor.invoke({\"input\": \"What team won the 2023 UEFA champions league? And 5+5=? And What is langchain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ff1b73-bba3-40ac-ad9f-a774b5f4a32f",
        "id": "Q-G29hQWMtze"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `tavily_search_results_json` with `{'query': 'Who won the 2023 UEFA Champions League?'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.sportingnews.com/us/soccer/news/who-won-champions-league-final-2023-man-city-inter-treble-rodri/ypj0yxkofcr9ki7zfnsi7i6c', 'content': \"MORE: Enjoy a minute-by-minute recap of the 2023 Champions League final as City win the European title\\nMan City win 2023 Champions League final\\nPep Guardiola and Manchester City have finally summited the mountain they began climbing seven years ago, winning the club's first European crown by topping Inter Milan 1-0 in the 2023 UEFA Champions League final.\\n The former Manchester United star had a cruel own goal in the 2020 Europa League final with Inter in defeat to Sevilla, and then suffered an outrageous string of misses with Belgium in the 2022 World Cup at the end of their eventual defeat to Croatia in the group stage.\\n Rodri gets breakthrough goal for Man City\\nFinally, the moment the game was begging for came in the 68th minute as Rodri smashed home the opening goal of the match, putting Man City in front.\\n A goal from Rodri in the 68th minute was enough to do the job, as Inter put up a true challenge but failed to find the back of the net through the match.\\n With nobody in the area, Rodri pounced, charging onto the loose ball and blasting a shot that curled around two Inter defenders and into the back of the net, as Andre Onana was rooted to the spot.\\n\"}]\u001b[0m\u001b[32;1m\u001b[1;3mManchester City won the 2023 UEFA Champions League. 5+5=10. Langchain is a framework for developing applications powered by large language models. \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What team won the 2023 UEFA champions league? And 5+5=? And What is langchain?',\n",
              " 'output': 'Manchester City won the 2023 UEFA Champions League. 5+5=10. Langchain is a framework for developing applications powered by large language models. \\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "id": "Q-G29hQWMtze"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tavily-Search and Any Function"
      ],
      "metadata": {
        "id": "7N8_1Z8vYTKM"
      },
      "id": "7N8_1Z8vYTKM"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from langchain.tools.base import StructuredTool\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "import json\n",
        "\n",
        "class bmi(BaseModel): # Basemodel şablon belirleme sınıfını temsil eder.\n",
        "    height: float =Field(description=\"Height in metres of person\")\n",
        "    weight: float =Field(description=\"weight in kilograms of person\")\n",
        "\n",
        "\n",
        "def calculate_bmi(height: float, weight: float) -> float:\n",
        "    \"Calculates the Body Mass Index (BMI) given height in meters and weight in kilograms.\"\n",
        "    return weight / (height * height)\n",
        "\n",
        "\n",
        "calculate_bmi=StructuredTool(\n",
        "                              name='calculate_bmi_func',\n",
        "                              func=calculate_bmi,\n",
        "                              description=\"Calculates the Body Mass Index (BMI) given height in meters and weight in kilograms.\",\n",
        "                              args_schema=bmi\n",
        ")"
      ],
      "metadata": {
        "id": "pS0fVLk5yMWK"
      },
      "id": "pS0fVLk5yMWK",
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\", api_key=os.environ[\"LANGCHAIN\"])\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b451f433-3caa-41c6-e430-50d9fb2ebae8",
        "id": "P3kLjCLqAcXP"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "id": "P3kLjCLqAcXP"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0,\n",
        "                 model=\"gpt-4o-mini\",\n",
        "                 top_p=1.0)\n",
        "\n",
        "# The tools we'll give the Agent access to.\n",
        "tools=[search_tool, calculate_bmi]\n",
        "\n",
        "# Finally, let's create an openai agent with the tools, the language model, and the prompt\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "# agent_executor performs all the preparations necessary to execute a query.\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# Let's test it out!\n",
        "agent_executor.invoke({\"input\": \"\"\"Which team won the 2023 UEFA champions league? And What is the body mass index of a person \\\n",
        "whose height is 1.8 m and weight is 80000 g?\"\"\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9a9ad6-1b6c-45c8-c22f-96eeb39b6125",
        "id": "xUIm4ZLrAcXQ"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `tavily_search_results_json` with `{'query': '2023 UEFA Champions League winner'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.youtube.com/watch?v=9Qq0MJubzA0', 'content': 'Watch two minutes of behind-the-scenes content from the 2023 UEFA Champions League final, which saw Man City beat Inter to win the competition for the first time.'}]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `calculate_bmi_func` with `{'height': 1.8, 'weight': 80}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m24.691358024691358\u001b[0m\u001b[32;1m\u001b[1;3mThe winner of the 2023 UEFA Champions League was Manchester City, who defeated Inter Milan to win the competition for the first time. You can watch a behind-the-scenes video [here](https://www.youtube.com/watch?v=9Qq0MJubzA0).\n",
            "\n",
            "The Body Mass Index (BMI) of a person with a height of 1.8 meters and a weight of 80 kilograms (80,000 grams) is approximately 24.69.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Which team won the 2023 UEFA champions league? And What is the body mass index of a person whose height is 1.8 m and weight is 80000 g?',\n",
              " 'output': 'The winner of the 2023 UEFA Champions League was Manchester City, who defeated Inter Milan to win the competition for the first time. You can watch a behind-the-scenes video [here](https://www.youtube.com/watch?v=9Qq0MJubzA0).\\n\\nThe Body Mass Index (BMI) of a person with a height of 1.8 meters and a weight of 80 kilograms (80,000 grams) is approximately 24.69.'}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "id": "xUIm4ZLrAcXQ"
    },
    {
      "cell_type": "markdown",
      "id": "09cd3a12",
      "metadata": {
        "id": "09cd3a12"
      },
      "source": [
        "### Wikipedia and Any Function tool"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU wikipedia"
      ],
      "metadata": {
        "id": "X0Y_n6sCf3Zm"
      },
      "id": "X0Y_n6sCf3Zm",
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "wiki_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=1000))"
      ],
      "metadata": {
        "id": "50dilH2IL_S4"
      },
      "id": "50dilH2IL_S4",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
        "#from langchain.tools import Tool\n",
        "\n",
        "#wiki = WikipediaAPIWrapper(top_k_results=1)\n",
        "\n",
        "# We manually create a tool that performs wikipedia search.\n",
        "#wiki_tool = Tool(\n",
        "#    name=\"wikipedia_search\",\n",
        "#    description=\"Search wikipedia for recent results.\",\n",
        "#    func=wiki.run\n",
        "#)"
      ],
      "metadata": {
        "id": "rk7Fb0bRe6IA"
      },
      "id": "rk7Fb0bRe6IA",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")"
      ],
      "metadata": {
        "id": "ZlP5ioaLYZ0k"
      },
      "execution_count": 81,
      "outputs": [],
      "id": "ZlP5ioaLYZ0k"
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_tool.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VuBgkjQqDmbP",
        "outputId": "b73c3526-f9fa-49fd-a3e0-4d4cbb28889c"
      },
      "id": "VuBgkjQqDmbP",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wikipedia'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_bmi.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "F8j6MXGeKsc2",
        "outputId": "07c9cf86-ea13-4cbd-d243-575ee3586f8b"
      },
      "id": "F8j6MXGeKsc2",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'calculate_bmi_func'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Make sure to use the wikipedia tool for information. And make sure to use calculate_bmi_func tool for body mass index calculations\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\")\n",
        "    ]\n",
        ")\n",
        "tools=[wiki_tool, calculate_bmi]\n",
        "# Construct the Tools agent\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "agent_executor.invoke({\"input\": \"What is the body mass index of a person \\\n",
        "whose height is 1.8 m and weight is 80000 g?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POv1v_97YZ0k",
        "outputId": "b8e65682-6485-411e-e726-39286462e555"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `calculate_bmi_func` with `{'weight': 80.0, 'height': 1.8}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m24.691358024691358\u001b[0m\u001b[32;1m\u001b[1;3mThe body mass index of a person whose height is 1.8 m and weight is 80 kg is 24.69. \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the body mass index of a person whose height is 1.8 m and weight is 80000 g?',\n",
              " 'output': 'The body mass index of a person whose height is 1.8 m and weight is 80 kg is 24.69. \\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "id": "POv1v_97YZ0k"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test it out!\n",
        "agent_executor.invoke({\"input\": \"When was Albert Einstein born? If he was alive, how old would he be right now in 2024?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7cbab50-04a1-48f7-ca17-1b6b27c6c1ab",
        "id": "8DBaFp7VgDeD"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `wikipedia` with `{'query': 'Albert Einstein'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Albert Einstein\n",
            "Summary: Albert Einstein ( EYEN-styne; German: [ˈalbɛɐt ˈʔaɪnʃtaɪn] ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist who is widely held as one of the most influential scientists. Best known for developing the theory of relativity, Einstein also made important contributions to quantum mechanics. His mass–energy equivalence formula E = mc2, which arises from special relativity, has been called \"the world's most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. \n",
            "Born in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship (as a subject of the Kingdom of Württemberg) the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss federal polytechnic school in \u001b[0m\u001b[32;1m\u001b[1;3mAlbert Einstein was born on March 14, 1879. If he was alive today in 2024, he would be 145 years old. \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'When was Albert Einstein born? If he was alive, how old would he be right now in 2024?',\n",
              " 'output': 'Albert Einstein was born on March 14, 1879. If he was alive today in 2024, he would be 145 years old. \\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "id": "8DBaFp7VgDeD"
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.memory"
      ],
      "metadata": {
        "id": "cvw8807_4GR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd510c2c-4693-426d-e908-282d96700b1b"
      },
      "id": "cvw8807_4GR_",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agents with Memory"
      ],
      "metadata": {
        "id": "KY31J0Gv3xL4"
      },
      "id": "KY31J0Gv3xL4"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Make sure to use the wikipedia tool for information. And make sure to use calculate_bmi tool for body mass index calculations\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "krcnOrfHZYzh"
      },
      "id": "krcnOrfHZYzh",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "def agent_history(user):\n",
        "  from langchain_core.messages import AIMessage, HumanMessage\n",
        "  from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "  global chat_history\n",
        "  llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "  # Finally, let's create an openai agent with the tools, the language model, and the prompt\n",
        "  agent = create_tool_calling_agent(llm, tools, prompt=prompt)\n",
        "\n",
        "  # agent_executor performs all the preparations necessary to execute a query.\n",
        "  agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
        "\n",
        "  input_message = {\"input\": user, \"chat_history\": chat_history}\n",
        "\n",
        "  # Let's test it out!\n",
        "  response= agent_executor.invoke(input_message)\n",
        "\n",
        "  chat_history+=[HumanMessage(content=user)]\n",
        "  chat_history+=[AIMessage(content=response[\"output\"])]\n",
        "  chat_history=chat_history[-4:]\n",
        "  return response[\"output\"]\n"
      ],
      "metadata": {
        "id": "cnSIVBuWIKfd"
      },
      "id": "cnSIVBuWIKfd",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_history(\"'When was Albert Einstein born?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "iZgimrapI1jq",
        "outputId": "4c52e002-6645-480b-ee98-c735abae8f80"
      },
      "id": "iZgimrapI1jq",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Albert Einstein was born on March 14, 1879. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_history(\"So when did he die?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "gg_jfbXxMa_r",
        "outputId": "5d928c5d-aada-4019-81fe-3b2bdad92924"
      },
      "id": "gg_jfbXxMa_r",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Albert Einstein died on April 18, 1955. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis [Toolkit](https://python.langchain.com/docs/integrations/toolkits/csv/)\n",
        "\n",
        "## [Langchain Toolkits](https://python.langchain.com/docs/integrations/toolkits/)"
      ],
      "metadata": {
        "id": "RNModKU0bvFP"
      },
      "id": "RNModKU0bvFP"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_experimental"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79c6caa-d6aa-4e77-9db5-b89402e7e202",
        "id": "aTatPjPiaan4"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/204.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m194.6/204.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.3/204.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "id": "aTatPjPiaan4"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
        "\n",
        "llm=ChatOpenAI(temperature=0.0,model=\"gpt-4o-mini\")\n",
        "\n",
        "agent = create_csv_agent(\n",
        "    llm,\n",
        "    \"/content/Advertising.csv\",\n",
        "    verbose=True,\n",
        "    agent_type=AgentType.OPENAI_FUNCTIONS, #ZERO_SHOT_REACT_DESCRIPTION\n",
        "    allow_dangerous_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "yidZiu41aan5"
      },
      "execution_count": 68,
      "outputs": [],
      "id": "yidZiu41aan5"
    },
    {
      "cell_type": "code",
      "source": [
        "input=\"What is the average of advertisements on TV?\"\n",
        "agent.invoke({\"input\":input})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NbHVAl_qUl0",
        "outputId": "4e429fa8-5ea8-4faa-f5f7-a61dccf93f44"
      },
      "id": "1NbHVAl_qUl0",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"df['TV'].mean()\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m147.0425\u001b[0m\u001b[32;1m\u001b[1;3mThe average of advertisements on TV is 147.04.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the average of advertisements on TV?',\n",
              " 'output': 'The average of advertisements on TV is 147.04.'}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"What is the median of advertisements on radio?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f438d7ca-ca95-479b-9f06-277daa1d3fb6",
        "id": "HtwEsfHXaan5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"df['radio'].median()\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m22.9\u001b[0m\u001b[32;1m\u001b[1;3mThe median of advertisements on radio is 22.9.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the median of advertisements on radio?',\n",
              " 'output': 'The median of advertisements on radio is 22.9.'}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "id": "HtwEsfHXaan5"
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"What is the average sales of 20 or more commercials on TV and 5 or more commercials on radio?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1GL-a25iaJU",
        "outputId": "864f6925-8e7c-49f9-a5d4-8afa64fd7420"
      },
      "id": "q1GL-a25iaJU",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"average_sales = df[(df['TV'] >= 20) & (df['radio'] >= 5)]['sales'].mean()\\naverage_sales\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m15.627272727272729\u001b[0m\u001b[32;1m\u001b[1;3mThe average sales for commercials with 20 or more on TV and 5 or more on radio is approximately 15.63.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the average sales of 20 or more commercials on TV and 5 or more commercials on radio?',\n",
              " 'output': 'The average sales for commercials with 20 or more on TV and 5 or more on radio is approximately 15.63.'}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/Advertising.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "3KmFM6ErifPG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "f892aad4-6ee5-4850-aed1-e6b58e42a9f5"
      },
      "id": "3KmFM6ErifPG",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        TV  radio  newspaper  sales\n",
              "0    230.1   37.8       69.2   22.1\n",
              "1     44.5   39.3       45.1   10.4\n",
              "2     17.2   45.9       69.3    9.3\n",
              "3    151.5   41.3       58.5   18.5\n",
              "4    180.8   10.8       58.4   12.9\n",
              "..     ...    ...        ...    ...\n",
              "195   38.2    3.7       13.8    7.6\n",
              "196   94.2    4.9        8.1    9.7\n",
              "197  177.0    9.3        6.4   12.8\n",
              "198  283.6   42.0       66.2   25.5\n",
              "199  232.1    8.6        8.7   13.4\n",
              "\n",
              "[200 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e58a374-984d-431a-94af-7d1b7b0d13ac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TV</th>\n",
              "      <th>radio</th>\n",
              "      <th>newspaper</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>230.1</td>\n",
              "      <td>37.8</td>\n",
              "      <td>69.2</td>\n",
              "      <td>22.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44.5</td>\n",
              "      <td>39.3</td>\n",
              "      <td>45.1</td>\n",
              "      <td>10.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17.2</td>\n",
              "      <td>45.9</td>\n",
              "      <td>69.3</td>\n",
              "      <td>9.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>151.5</td>\n",
              "      <td>41.3</td>\n",
              "      <td>58.5</td>\n",
              "      <td>18.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>180.8</td>\n",
              "      <td>10.8</td>\n",
              "      <td>58.4</td>\n",
              "      <td>12.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>38.2</td>\n",
              "      <td>3.7</td>\n",
              "      <td>13.8</td>\n",
              "      <td>7.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>94.2</td>\n",
              "      <td>4.9</td>\n",
              "      <td>8.1</td>\n",
              "      <td>9.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>177.0</td>\n",
              "      <td>9.3</td>\n",
              "      <td>6.4</td>\n",
              "      <td>12.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>283.6</td>\n",
              "      <td>42.0</td>\n",
              "      <td>66.2</td>\n",
              "      <td>25.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>232.1</td>\n",
              "      <td>8.6</td>\n",
              "      <td>8.7</td>\n",
              "      <td>13.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e58a374-984d-431a-94af-7d1b7b0d13ac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6e58a374-984d-431a-94af-7d1b7b0d13ac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6e58a374-984d-431a-94af-7d1b7b0d13ac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-78f10477-f6a3-4160-908e-bd75367e421e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-78f10477-f6a3-4160-908e-bd75367e421e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-78f10477-f6a3-4160-908e-bd75367e421e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2e5ecd61-c4ec-4cf1-8c10-afe5c774549a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2e5ecd61-c4ec-4cf1-8c10-afe5c774549a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"TV\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 85.85423631490808,\n        \"min\": 0.7,\n        \"max\": 296.4,\n        \"num_unique_values\": 190,\n        \"samples\": [\n          287.6,\n          286.0,\n          78.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"radio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.846809176168724,\n        \"min\": 0.0,\n        \"max\": 49.6,\n        \"num_unique_values\": 167,\n        \"samples\": [\n          8.2,\n          36.9,\n          44.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"newspaper\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21.778620838522826,\n        \"min\": 0.3,\n        \"max\": 114.0,\n        \"num_unique_values\": 172,\n        \"samples\": [\n          22.3,\n          5.7,\n          17.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sales\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.217456565710478,\n        \"min\": 1.6,\n        \"max\": 27.0,\n        \"num_unique_values\": 121,\n        \"samples\": [\n          11.4,\n          21.2,\n          12.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df['TV'] >= 20) & (df['radio'] >= 5)].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "NJ3G92_-tk23",
        "outputId": "cff10c93-c8aa-46a6-b4a0-3255a1e1501f"
      },
      "id": "NJ3G92_-tk23",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TV           161.919481\n",
              "radio         26.636364\n",
              "newspaper     31.531169\n",
              "sales         15.627273\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>TV</th>\n",
              "      <td>161.919481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radio</th>\n",
              "      <td>26.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>newspaper</th>\n",
              "      <td>31.531169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sales</th>\n",
              "      <td>15.627273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V76SnlVPtv8Q"
      },
      "id": "V76SnlVPtv8Q",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}