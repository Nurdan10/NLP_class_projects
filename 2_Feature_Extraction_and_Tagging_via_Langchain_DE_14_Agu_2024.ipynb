{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b84edb4e",
      "metadata": {
        "id": "b84edb4e"
      },
      "source": [
        "# Feature Extraction and Tagging\n",
        "\n",
        "## Use case\n",
        "\n",
        "Getting structured output from raw LLM generations is hard.\n",
        "\n",
        "For example, suppose you need the model output formatted with a specific schema for:\n",
        "\n",
        "- Extracting different parts of a user query (e.g., for semantic vs keyword search)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "178dbc59",
      "metadata": {
        "id": "178dbc59"
      },
      "source": [
        "![Image description](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/extraction.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97f474d4",
      "metadata": {
        "id": "97f474d4"
      },
      "source": [
        "## Overview\n",
        "\n",
        "There are two primary approaches for this:\n",
        "\n",
        "- `Functions`: Some LLMs can call [functions](https://openai.com/blog/function-calling-and-other-api-updates) to extract arbitrary entities from LLM responses.\n",
        "\n",
        "- `Pydantic`: Pydantic library is used to extract the features we want from the data for Python.\n",
        "\n",
        "Only some LLMs support functions (e.g., OpenAI), and they are more general than parsers.\n",
        "\n",
        "Parsers extract precisely what is enumerated in a provided schema (e.g., specific attributes of a person).\n",
        "\n",
        "Functions can infer things beyond of a provided schema (e.g., attributes about a person that you did not ask for)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25d89f21",
      "metadata": {
        "id": "25d89f21"
      },
      "source": [
        "## Quickstart\n",
        "\n",
        "OpenAI functions are one way to get started with extraction.\n",
        "\n",
        "Define a schema that specifies the properties we want to extract from the LLM output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f5ec7a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f5ec7a3",
        "outputId": "71261358-08c1-42e3-f02e-faedd0306ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m580.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.8/384.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.3/361.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "mnkeqTm5xymm"
      },
      "id": "mnkeqTm5xymm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Pydantic with \"with_structured_output\" Method](https://python.langchain.com/v0.1/docs/use_cases/extraction/how_to/examples/)"
      ],
      "metadata": {
        "id": "IizJXt6wDsP7"
      },
      "id": "IizJXt6wDsP7"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "inp= 'Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette.'\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    # ^ Doc-string for the entity Person.\n",
        "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
        "    # and it can help to improve extraction results.\n",
        "\n",
        "    # Note that:\n",
        "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
        "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
        "    # Having a good description can help improve extraction results.\n",
        "    name: Optional[str] = Field(description=\"The person's name\", default=None)\n",
        "    height: Optional[float] = Field(description=\"Height measured in feets\", default=None)\n",
        "    hair_color: Optional[str] = Field(description=\"The color of the person's hair if known\",default=None)\n",
        "\n",
        "class Data(BaseModel):\n",
        "    \"\"\"Extracted data about people.\"\"\"\n",
        "\n",
        "    # Creates a model so that we can extract multiple entities.\n",
        "    people: List[Person]= Field(description=\"List of people mentioned in the text.\")"
      ],
      "metadata": {
        "id": "pdzTE8z7D5tB"
      },
      "id": "pdzTE8z7D5tB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Define a custom prompt to provide instructions and any additional context.\n",
        "# 1) You can add examples into the prompt template to improve extraction quality\n",
        "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
        "#    about the document from which the text was extracted.)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert extraction algorithm. \"\n",
        "        ),\n",
        "        # Please see the how-to about improving performance with\n",
        "        # reference examples.\n",
        "        # MessagesPlaceholder('examples'),\n",
        "        (\"human\",\n",
        "         \"Only extract relevant information from the '{text}'. \"\n",
        "         \"If you do not know the value of an attribute asked \"\n",
        "         \"to extract, return null for the attribute's value.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 temperature=0,\n",
        "                 top_p=1)"
      ],
      "metadata": {
        "id": "YSqANL43BP1i"
      },
      "id": "YSqANL43BP1i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NqJlM8CvEh7h",
        "outputId": "272d77b1-3a26-4df1-e253-5839fdb2f57a"
      },
      "id": "NqJlM8CvEh7h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We need to use a model that supports function/tool calling.\n",
        "chain = prompt | llm.with_structured_output(schema=Person) #if schema is Person, the output of model is for one person\n",
        "chain.invoke({\"text\":inp}) #inp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7pHItTFBP73",
        "outputId": "557a9e7c-c415-4f6e-ee03-864c6121163d"
      },
      "id": "Y7pHItTFBP73",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Person(name='Alex', height=5.0, hair_color=None)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm.with_structured_output(schema=Data) #if schema is Data, the output of model is for all person\n",
        "output= chain.invoke({\"text\":inp})\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGlY6wpHCm9M",
        "outputId": "3ed54207-5162-4d1c-c2a8-ba08b9da2b4c"
      },
      "id": "cGlY6wpHCm9M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[Person(name='Alex', height=5.0, hair_color=None), Person(name='Claudia', height=6.0, hair_color='brunette')])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.people[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR1ce5oCBQB_",
        "outputId": "ec90fc2a-72c9-452c-a7cf-26c37d8d3b1f"
      },
      "id": "RR1ce5oCBQB_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Person(name='Alex', height=5.0, hair_color=None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Another example via \"with_structured_output\" method\n",
        "\n",
        "#### Without Examples"
      ],
      "metadata": {
        "id": "sfojOwXZJvSt"
      },
      "id": "sfojOwXZJvSt"
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be using tool calling mode, which\n",
        "# requires a tool calling capable model.\n",
        "llm = ChatOpenAI(\n",
        "    # Consider benchmarking with a good model to get\n",
        "    # a sense of the best possible quality.\n",
        "    model=\"gpt-4o-mini\",\n",
        "    # Remember to set the temperature to 0 for extractions!\n",
        "    temperature=0,\n",
        "    top_p=1\n",
        ")\n",
        "\n",
        "\n",
        "runnable = prompt | llm.with_structured_output(schema=Person)"
      ],
      "metadata": {
        "id": "BylZziacBQHo"
      },
      "id": "BylZziacBQHo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    text = \"The solar system is large, but earth has only 1 moon.\"\n",
        "    print(runnable.invoke({\"text\": text, \"examples\": []}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6dBuMkWBQNA",
        "outputId": "64473e69-a813-49e5-afeb-0a847ac8ae3c"
      },
      "id": "S6dBuMkWBQNA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Earth' height=None hair_color=None\n",
            "name='Earth' height=None hair_color=None\n",
            "name='Earth' height=None hair_color=None\n",
            "name='Earth' height=None hair_color=None\n",
            "name='Earth' height=None hair_color=None\n",
            "name='Earth' height=None hair_color=None\n",
            "name='Earth' height=None hair_color=None\n",
            "name='Earth' height=None hair_color=None\n",
            "name='Earth' height=None hair_color=None\n",
            "name='Earth' height=None hair_color=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With examples"
      ],
      "metadata": {
        "id": "TvcDVxogIJeW"
      },
      "id": "TvcDVxogIJeW"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Define a custom prompt to provide instructions and any additional context.\n",
        "# 1) You can add examples into the prompt template to improve extraction quality\n",
        "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
        "#    about the document from which the text was extracted.)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert extraction algorithm.\"\n",
        "        ),\n",
        "        # ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "        MessagesPlaceholder(\"examples\"),  # <-- EXAMPLES!\n",
        "        # ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
        "        (\"human\",\n",
        "         \"Only extract relevant information from the '{text}'. \"\n",
        "         \"If you do not know the value of an attribute asked \"\n",
        "         \"to extract, return null for the attribute's value.\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "_wSwt0UAINMC"
      },
      "id": "_wSwt0UAINMC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "from typing import Dict, List, TypedDict\n",
        "\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class Example(TypedDict):\n",
        "    \"\"\"A representation of an example consisting of text input and expected tool calls.\n",
        "\n",
        "    For extraction, the tool calls are represented as instances of pydantic model.\n",
        "    \"\"\"\n",
        "\n",
        "    input: str  # This is the example text\n",
        "    tool_calls: List[BaseModel]  # Instances of pydantic model that should be extracted\n",
        "\n",
        "\n",
        "def tool_example_to_messages(example: Example) -> List[BaseMessage]:\n",
        "    \"\"\"Convert an example into a list of messages that can be fed into an LLM.\n",
        "\n",
        "    This code is an adapter that converts our example to a list of messages\n",
        "    that can be fed into a chat model.\n",
        "\n",
        "    The list of messages per example corresponds to:\n",
        "\n",
        "    1) HumanMessage: contains the content from which content should be extracted.\n",
        "    2) AIMessage: contains the extracted information from the model\n",
        "    3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.\n",
        "\n",
        "    The ToolMessage is required because some of the chat models are hyper-optimized for agents\n",
        "    rather than for an extraction use case.\n",
        "    \"\"\"\n",
        "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
        "    openai_tool_calls = []\n",
        "    for tool_call in example[\"tool_calls\"]:\n",
        "        openai_tool_calls.append(\n",
        "            {\n",
        "                \"id\": str(uuid.uuid4()),\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    # The name of the function right now corresponds\n",
        "                    # to the name of the pydantic model\n",
        "                    # This is implicit in the API right now,\n",
        "                    # and will be improved over time.\n",
        "                    \"name\": tool_call.__class__.__name__,\n",
        "                    \"arguments\": tool_call.json(),\n",
        "                },\n",
        "            }\n",
        "        )\n",
        "    messages.append(\n",
        "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
        "    )\n",
        "    tool_outputs = example.get(\"tool_outputs\") or [\n",
        "        \"You have correctly called this tool.\"\n",
        "    ] * len(openai_tool_calls)\n",
        "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
        "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
        "    return messages"
      ],
      "metadata": {
        "id": "RIpCiE9QIhgw"
      },
      "id": "RIpCiE9QIhgw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    (\n",
        "        \"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it.\",\n",
        "        Person(name=None, height=None, hair_color=None),\n",
        "    ),\n",
        "    (\n",
        "        \"Fiona traveled far from France to Spain.\",\n",
        "        Person(name=\"Fiona\", height=None, hair_color=None),\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "messages = []\n",
        "\n",
        "for text, tool_call in examples:\n",
        "    messages.extend(\n",
        "        tool_example_to_messages({\"input\": text, \"tool_calls\": [tool_call]})\n",
        "    )"
      ],
      "metadata": {
        "id": "QNR9s8WkIhnP"
      },
      "id": "QNR9s8WkIhnP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0NdEGi_Ihtv",
        "outputId": "9ee3e446-881c-4f41-fc0a-a59e49660104"
      },
      "id": "k0NdEGi_Ihtv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it.\"),\n",
              " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'e6004bcc-0273-4756-88b2-d01366db17f5', 'type': 'function', 'function': {'name': 'Person', 'arguments': '{\"name\": null, \"height\": null, \"hair_color\": null}'}}]}, tool_calls=[{'name': 'Person', 'args': {'name': None, 'height': None, 'hair_color': None}, 'id': 'e6004bcc-0273-4756-88b2-d01366db17f5', 'type': 'tool_call'}]),\n",
              " ToolMessage(content='You have correctly called this tool.', tool_call_id='e6004bcc-0273-4756-88b2-d01366db17f5'),\n",
              " HumanMessage(content='Fiona traveled far from France to Spain.'),\n",
              " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '5aed082c-20ea-4883-a885-84609035fda2', 'type': 'function', 'function': {'name': 'Person', 'arguments': '{\"name\": \"Fiona\", \"height\": null, \"hair_color\": null}'}}]}, tool_calls=[{'name': 'Person', 'args': {'name': 'Fiona', 'height': None, 'hair_color': None}, 'id': '5aed082c-20ea-4883-a885-84609035fda2', 'type': 'tool_call'}]),\n",
              " ToolMessage(content='You have correctly called this tool.', tool_call_id='5aed082c-20ea-4883-a885-84609035fda2')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    # Consider benchmarking with a good model to get\n",
        "    # a sense of the best possible quality.\n",
        "    model=\"gpt-4o-mini\",\n",
        "    # Remember to set the temperature to 0 for extractions!\n",
        "    temperature=0,\n",
        "    top_p=1\n",
        ")\n",
        "\n",
        "\n",
        "runnable = prompt | llm.with_structured_output(schema=Person)"
      ],
      "metadata": {
        "id": "juiAEbPzKYUe"
      },
      "id": "juiAEbPzKYUe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    text = \"The solar system is large, but earth has only 1 moon.\"\n",
        "    print(runnable.invoke({\"text\": text, \"examples\": messages}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33X6l-f6Ihz3",
        "outputId": "b80eb420-27ae-457b-a716-1219b02e50a3"
      },
      "id": "33X6l-f6Ihz3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name=None height=None hair_color=None\n",
            "name=None height=None hair_color=None\n",
            "name=None height=None hair_color=None\n",
            "name=None height=None hair_color=None\n",
            "name=None height=None hair_color=None\n",
            "name=None height=None hair_color=None\n",
            "name=None height=None hair_color=None\n",
            "name=None height=None hair_color=None\n",
            "name=None height=None hair_color=None\n",
            "name=None height=None hair_color=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb03138",
      "metadata": {
        "id": "dcb03138"
      },
      "source": [
        "### Multiple entity types\n",
        "\n",
        "We can extend this further.\n",
        "\n",
        "Let's say we want to differentiate between dogs and people.\n",
        "\n",
        "We can add `person_` and `dog_` prefixes for each property"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Define a custom prompt to provide instructions and any additional context.\n",
        "# 1) You can add examples into the prompt template to improve extraction quality\n",
        "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
        "#    about the document from which the text was extracted.)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert extraction algorithm. \"\n",
        "        ),\n",
        "        # Please see the how-to about improving performance with\n",
        "        # reference examples.\n",
        "        # MessagesPlaceholder('examples'),\n",
        "        (\"human\",\n",
        "         \"Only extract relevant information from the '{text}'. \"\n",
        "         \"If you do not know the value of an attribute asked \"\n",
        "         \"to extract, return null for the attribute's value.\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "w8UhfUBhfD84"
      },
      "id": "w8UhfUBhfD84",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "class Person_and_Dog(BaseModel):\n",
        "    \"\"\"Information about a person and dog.\"\"\"\n",
        "\n",
        "    person_name: Optional[str] = Field(description=\"The person's name\", default=None)\n",
        "    person_height: Optional[float] = Field(description=\"Height measured in feets\", default=None)\n",
        "    person_hair_color: Optional[str] = Field(description=\"The color of the person's hair if known\", default=None)\n",
        "    dog_name: Optional[str] = Field(description=\"The name of the dog\", default=None)\n",
        "    dog_breed: Optional[str] = Field(description=\"The breed of the dog\", default=None)\n",
        "\n",
        "class Data(BaseModel):\n",
        "    \"\"\"Extracted data about people and dogs.\"\"\"\n",
        "\n",
        "    # Creates a model so that we can extract multiple entities.\n",
        "    people_dogs: List[Person_and_Dog]= Field(description=\"List of people and dogs mentioned in the text.\")"
      ],
      "metadata": {
        "id": "sU6ynERISPai"
      },
      "id": "sU6ynERISPai",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 temperature=0,\n",
        "                 top_p=1)\n",
        "inp = \"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\n",
        "chain = prompt | llm.with_structured_output(schema=Data)\n",
        "chain.invoke({\"text\":inp}).people_dogs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifaaj--wTWmx",
        "outputId": "ddb86cab-4c02-4802-afd6-ae36d104c65c"
      },
      "id": "ifaaj--wTWmx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Person_and_Dog(person_name='Alex', person_height=5.0, person_hair_color='blonde', dog_name=None, dog_breed=None),\n",
              " Person_and_Dog(person_name='Claudia', person_height=6.0, person_hair_color='brunette', dog_name=None, dog_breed=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\n",
        "Alex's dog Frosty is a labrador and likes to play hide and seek.\"\"\"\n",
        "\n",
        "chain.invoke({\"text\":inp}).people_dogs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cJF08rtUCwh",
        "outputId": "448f8964-d98b-48cf-e008-a30693325c25"
      },
      "id": "9cJF08rtUCwh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Person_and_Dog(person_name='Alex', person_height=5.0, person_hair_color='blonde', dog_name='Frosty', dog_breed='labrador'),\n",
              " Person_and_Dog(person_name='Claudia', person_height=6.0, person_hair_color='brunette', dog_name=None, dog_breed=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\n",
        "Willow is a German Shepherd that likes to play with other dogs and can always be found playing with Milo, a border collie that lives close by.\"\"\"\n",
        "\n",
        "output= chain.invoke({\"text\":inp})\n",
        "output.people_dogs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCIRBxPMVtTI",
        "outputId": "9274a85f-4424-481b-afb4-13153cbfe273"
      },
      "id": "aCIRBxPMVtTI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Person_and_Dog(person_name='Alex', person_height=5.0, person_hair_color='blonde', dog_name=None, dog_breed=None),\n",
              " Person_and_Dog(person_name='Claudia', person_height=6.0, person_hair_color='brunette', dog_name=None, dog_breed=None),\n",
              " Person_and_Dog(person_name=None, person_height=None, person_hair_color=None, dog_name='Willow', dog_breed='German Shepherd'),\n",
              " Person_and_Dog(person_name=None, person_height=None, person_hair_color=None, dog_name='Milo', dog_breed='border collie')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34f3b958",
      "metadata": {
        "id": "34f3b958"
      },
      "source": [
        "### Extra information\n",
        "\n",
        "The power of functions (relative to using parsers alone) lies in the ability to perform semantic extraction.\n",
        "\n",
        "In particular, `we can ask for things that are not explicitly enumerated in the schema`.\n",
        "\n",
        "Suppose we want unspecified additional information about dogs.\n",
        "\n",
        "We can use add a placeholder for unstructured extraction, `dog_extra_info`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Person_and_Dog(BaseModel):\n",
        "    \"\"\"Information about a person and dog.\"\"\"\n",
        "\n",
        "    person_name: Optional[str] = Field(description=\"The person's name\", default=None)\n",
        "    person_height: Optional[float] = Field(description=\"Height measured in feets\", default=None)\n",
        "    person_hair_color: Optional[str] = Field(description=\"The color of the person's hair if known\", default=None)\n",
        "    dog_name: Optional[str] = Field(description=\"The name of the dog\", default=None)\n",
        "    dog_breed: Optional[str] = Field(description=\"The breed of the dog\", default=None)\n",
        "    dog_extra_info: Optional[str] = Field(description=\"extra information about the dog\", default=None)\n",
        "\n",
        "class Data(BaseModel):\n",
        "    \"\"\"Extracted data about people and dogs.\"\"\"\n",
        "\n",
        "    # Creates a model so that we can extract multiple entities.\n",
        "    people_dogs: List[Person_and_Dog]= Field(description=\"List of people and dogs mentioned in the text.\")"
      ],
      "metadata": {
        "id": "iG3KxbO1ZKY_"
      },
      "id": "iG3KxbO1ZKY_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 temperature=0,\n",
        "                 top_p=1)"
      ],
      "metadata": {
        "id": "ZQTIaceQapPR"
      },
      "id": "ZQTIaceQapPR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert extraction algorithm. \"\n",
        "        ),\n",
        "        # Please see the how-to about improving performance with\n",
        "        # reference examples.\n",
        "        # MessagesPlaceholder('examples'),\n",
        "        (\"human\",\n",
        "         \"Only extract relevant information from the '{text}'. \"\n",
        "         \"If you do not know the value of an attribute asked \"\n",
        "         \"to extract, return null for the attribute's value.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "runnable = prompt | llm.with_structured_output(schema=Data)"
      ],
      "metadata": {
        "id": "M08POsruZ0lY"
      },
      "id": "M08POsruZ0lY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\n",
        "Willow is a German Shepherd that likes to play with other dogs and can always be found playing with Milo, a border collie that lives close by.\"\"\"\n",
        "#Willow, diğer köpeklerle oynamayı seven bir Alman Çoban Köpeğidir ve her zaman yakınlarda yaşayan border collie cinsi Milo ile oynarken bulunabilir.\n",
        "output= runnable.invoke({\"text\":inp})\n",
        "output.people_dogs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaNB_pANZ0Yh",
        "outputId": "070c00f6-5eec-4442-c65b-0ad1ff71c1f2"
      },
      "id": "KaNB_pANZ0Yh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Person_and_Dog(person_name='Alex', person_height=5.0, person_hair_color='blonde', dog_name=None, dog_breed=None, dog_extra_info=None),\n",
              " Person_and_Dog(person_name='Claudia', person_height=6.0, person_hair_color='brunette', dog_name=None, dog_breed=None, dog_extra_info=None),\n",
              " Person_and_Dog(person_name=None, person_height=None, person_hair_color=None, dog_name='Willow', dog_breed='German Shepherd', dog_extra_info='likes to play with other dogs'),\n",
              " Person_and_Dog(person_name=None, person_height=None, person_hair_color=None, dog_name='Milo', dog_breed='border collie', dog_extra_info='lives close by')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a949c60",
      "metadata": {
        "id": "3a949c60"
      },
      "source": [
        "This gives us additional information about the dogs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Tagging](https://python.langchain.com/v0.1/docs/use_cases/tagging/)"
      ],
      "metadata": {
        "id": "pi1f5olDHkPK"
      },
      "id": "pi1f5olDHkPK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Use case\n",
        "\n",
        "Tagging means labeling a document with classes such as:\n",
        "\n",
        "- sentiment\n",
        "- language\n",
        "- style (formal, informal etc.)\n",
        "- covered topics\n",
        "- political tendency\n",
        "\n",
        "![Image description](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/tagging.png?raw=1)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Tagging has a few components:\n",
        "\n",
        "* `function`: tagging uses [functions](https://openai.com/blog/function-calling-and-other-api-updates) to specify how the model should tag a document\n",
        "* `schema and pydantic`: defines how we want to tag the document\n",
        "\n",
        "## Quickstart\n",
        "\n",
        "Let's see a very straightforward example of how we can use OpenAI functions for tagging in LangChain."
      ],
      "metadata": {
        "id": "dyhF52ReHmeS"
      },
      "id": "dyhF52ReHmeS"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_tagging_chain\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "X7mPqvz3JNHe"
      },
      "id": "X7mPqvz3JNHe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We specify a few properties with their expected type in our schema."
      ],
      "metadata": {
        "id": "_wU0uRF1JTDP"
      },
      "id": "_wU0uRF1JTDP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Schema\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"sentiment\": {\"type\": \"string\"},\n",
        "        \"aggressiveness\": {\"type\": \"integer\"},\n",
        "        \"language\": {\"type\": \"string\"},\n",
        "    }\n",
        "}\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model=\"gpt-3.5-turbo-0125\",\n",
        "                 #openai_api_key=os.environ['OPENAI_API_KEY']\n",
        "                 )\n",
        "chain = create_tagging_chain(schema, llm)"
      ],
      "metadata": {
        "id": "gzKEswJYJTw3"
      },
      "id": "gzKEswJYJTw3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\n",
        "chain.invoke(inp)[\"text\"] # Usually don't return aggressiveness because required is not specified"
      ],
      "metadata": {
        "id": "dM2iOE_fJZ3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8d08ce-ce3b-42d1-cf73-a672989b22d9"
      },
      "id": "dM2iOE_fJZ3d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentiment': 'positive', 'language': 'Spanish'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\n",
        "chain.invoke(inp)[\"text\"]"
      ],
      "metadata": {
        "id": "0TUE1HKaJgu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "637bcecf-df5b-46f8-d2f6-c27ba0ef3618"
      },
      "id": "0TUE1HKaJgu3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentiment': 'enojado', 'aggressiveness': 3, 'language': 'Spanish'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see in the examples, it correctly interprets what we want.\n",
        "\n",
        "The results vary so that we get, for example, sentiments in different languages ('positive', 'enojado' etc.).\n",
        "\n",
        "We will see how to control these results in the next section."
      ],
      "metadata": {
        "id": "HWWxEvKYJn4h"
      },
      "id": "HWWxEvKYJn4h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finer control\n",
        "\n",
        "Careful schema definition gives us more control over the model's output.\n",
        "\n",
        "Specifically, we can define:\n",
        "\n",
        "- possible values for each property\n",
        "- description to make sure that the model understands the property\n",
        "- required properties to be returned\n",
        "\n",
        "Here is an example of how we can use `_enum_`, `_description_`, and `_required_` to control for each of the previously mentioned aspects:"
      ],
      "metadata": {
        "id": "xQOsvgU2JtoM"
      },
      "id": "xQOsvgU2JtoM"
    },
    {
      "cell_type": "code",
      "source": [
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"sentiment\": {\"type\": \"string\",\n",
        "                      \"enum\": [\"positive\", \"neutral\", \"negative\"],\n",
        "                      \"description\": \"The sentiment for text\"},\n",
        "        \"aggressiveness\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"enum\": [1, 2, 3, 4, 5],\n",
        "            \"description\": \"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
        "        },\n",
        "        \"language\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"spanish\", \"english\", \"french\", \"german\", \"italian\"],\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"language\", \"sentiment\", \"aggressiveness\"],\n",
        "}"
      ],
      "metadata": {
        "id": "KmC2A64sJuf-"
      },
      "id": "KmC2A64sJuf-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model=\"gpt-3.5-turbo-0125\")\n",
        "chain = create_tagging_chain(schema, llm)"
      ],
      "metadata": {
        "id": "JVYbowOTJ7Ie"
      },
      "id": "JVYbowOTJ7Ie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the answers are much better!"
      ],
      "metadata": {
        "id": "dC1QL_-wJ-nG"
      },
      "id": "dC1QL_-wJ-nG"
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\" #\"Seninle tanıştığım için inanılmaz mutlu oldum! Sanırım çok iyi arkadaş olacağız!\"\n",
        "chain.invoke(inp)[\"text\"]"
      ],
      "metadata": {
        "id": "NVGzTwrCJ9uk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed4d4860-a646-4ede-84de-d04e805de4b7"
      },
      "id": "NVGzTwrCJ9uk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentiment': 'positive', 'aggressiveness': 1, 'language': 'spanish'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"Weather is ok here, I can go outside without much more than a coat\" #\"Burada hava güzel, paltodan başka bir şey olmadan dışarı çıkabiliyorum\"\n",
        "chain.invoke(inp)[\"text\"]"
      ],
      "metadata": {
        "id": "YjVs1S9nKHTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c71cd2-9093-402e-983f-c0e3abc6584c"
      },
      "id": "YjVs1S9nKHTA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentiment': 'neutral', 'aggressiveness': 1, 'language': 'english'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pydantic"
      ],
      "metadata": {
        "id": "e0ehhgjOjaT6"
      },
      "id": "e0ehhgjOjaT6"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.chains import create_tagging_chain_pydantic\n",
        "\n",
        "\n",
        "\n",
        "class Classification(BaseModel):\n",
        "    sentiment: str = Field(description=\"The sentiment of the text\",\n",
        "                             enum=[\"happy\", \"neutral\", \"sad\"])\n",
        "\n",
        "    aggressiveness: int = Field(description=\"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
        "                            enum=[1, 2, 3, 4, 5])\n",
        "\n",
        "    language: str = Field(description=\"The language the text is written in\",\n",
        "                            enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"])\n",
        "\n",
        "inp = \"Weather is ok here, I can go outside without much more than a coat\"\n",
        "chain2 = create_tagging_chain_pydantic(Classification, llm)\n",
        "chain2.invoke(inp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZHukbrkjaxF",
        "outputId": "c2737c6a-1867-4970-be83-22ef18ee4206"
      },
      "id": "HZHukbrkjaxF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` that is available on ChatModels capable of tool calling. See API reference for this function for replacement: <https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.tagging.create_tagging_chain_pydantic.html> You can read more about `with_structured_output` here: <https://python.langchain.com/v0.2/docs/how_to/structured_output/>. If you notice other issues, please provide feedback here: <https://github.com/langchain-ai/langchain/discussions/18154>\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Weather is ok here, I can go outside without much more than a coat',\n",
              " 'text': Classification(sentiment='neutral', aggressiveness=1, language='english')}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"damn!\"\n",
        "chain2.invoke(inp)[\"text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHpUSqN_ja3S",
        "outputId": "dca12f84-3acc-4b19-a25d-a701a388271b"
      },
      "id": "FHpUSqN_ja3S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classification(sentiment='sad', aggressiveness=5, language='english')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ9c3pO_n1Qd",
        "outputId": "5d263fcf-0578-4fd2-8451-32d7e81d8b75"
      },
      "id": "wJ9c3pO_n1Qd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template=\"Extract the desired information from the following passage.\\n\\nOnly extract the properties mentioned in the 'information_extraction' function.\\n\\nPassage:\\n{input}\\n\"))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7c681e11ae30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7c681e1025f0>, root_client=<openai.OpenAI object at 0x7c681e11a9b0>, root_async_client=<openai.AsyncOpenAI object at 0x7c681e11ae60>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), output_parser=PydanticOutputFunctionsParser(pydantic_schema=<class '__main__.Classification'>), llm_kwargs={'functions': [{'name': 'information_extraction', 'description': 'Extracts the relevant information from the passage.', 'parameters': {'type': 'object', 'properties': {'sentiment': {'title': 'Sentiment', 'description': 'The sentiment of the text', 'enum': ['happy', 'neutral', 'sad'], 'type': 'string'}, 'aggressiveness': {'title': 'Aggressiveness', 'description': 'describes how aggressive the statement is, the higher the number the more aggressive', 'enum': [1, 2, 3, 4, 5], 'type': 'integer'}, 'language': {'title': 'Language', 'description': 'The language the text is written in', 'enum': ['spanish', 'english', 'french', 'german', 'italian'], 'type': 'string'}}, 'required': ['sentiment', 'aggressiveness', 'language']}}], 'function_call': {'name': 'information_extraction'}})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain2.prompt.messages[0].prompt.template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aGjtJXKw4Xgn",
        "outputId": "e19f56bb-901e-4200-ed33-a8bc8f7a2e20"
      },
      "id": "aGjtJXKw4Xgn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Extract the desired information from the following passage.\\n\\nOnly extract the properties mentioned in the 'information_extraction' function.\\n\\nPassage:\\n{input}\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with_structured_output"
      ],
      "metadata": {
        "id": "RjLjKJ6t2Ek_"
      },
      "id": "RjLjKJ6t2Ek_"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "tagging_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Extract the desired information from the following passage.\n",
        "\n",
        "Only extract the properties mentioned in the 'Classification' function.\n",
        "\n",
        "Passage:\n",
        "{input}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "inp = \"damn!\"\n",
        "\n",
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model=\"gpt-4o-mini\")\n",
        "\n",
        "tagging_chain = tagging_prompt | llm.with_structured_output(Classification)\n",
        "tagging_chain.invoke({\"input\": inp})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMBbQTyJfJrx",
        "outputId": "145f055c-7ee0-4ff6-a81c-7c77dd633fca"
      },
      "id": "PMBbQTyJfJrx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classification(sentiment='sad', aggressiveness=4, language='english')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Label Classification"
      ],
      "metadata": {
        "id": "ERD3xSbGUoTv"
      },
      "id": "ERD3xSbGUoTv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make our example more specific and challenging. Let's analyze customers' comments separately for both the product they purchased and for their interactions with customer service."
      ],
      "metadata": {
        "id": "wN6alID7L-t9"
      },
      "id": "wN6alID7L-t9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Schema\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"sentiment_for_product\":{\"type\": \"string\",\n",
        "                                        \"enum\":[\"positive\", \"neutral\", \"negative\"],\n",
        "                                        \"description\": \"The sentiment for product\"},\n",
        "         \"sentiment_for_customer_service_issue\":{\"type\": \"string\",\n",
        "                                        \"enum\":[\"positive\", \"neutral\", \"negative\"],\n",
        "                                        \"description\": \"The sentiment for customer service issues\"},\n",
        "        \"tecnical_problems\": {\"type\": \"string\",\n",
        "                              \"description\": \"Details of the technical problem encountered with the products.\"},\n",
        "        \"negative_customer_surves_experiences\": {\"type\": \"string\",\n",
        "                                                 \"description\": \"Details of the negative customer service experiences\"}\n",
        "    },\n",
        "    \"required\": [\"sentiment_for_product\", \"sentiment_for_customer_service_issue\", \"tecnical_problems\",  \"negative_customer_surves_experiences\"]\n",
        "}\n",
        "\n",
        "# Input\n",
        "inp = [\"Although the phone's battery life is satisfactory, I had a frustrating experience with customer service when I needed help with a software issue.\",\n",
        "\"The camera's low-light performance is excellent, but I encountered difficulties with the phone's software updates. Fortunately, the customer service team was \\\n",
        "helpful in resolving the issue.\",\n",
        "\"The design of the phone is impressive, but I had to contact customer service multiple times to address an issue with the speaker.\",\n",
        "\"I'm satisfied with the phone's performance overall, but the lack of timely software updates is disappointing. Customer service was responsive when \\\n",
        "I reached out for assistance. \",\n",
        "\"The phone's sleek design caught my eye, but I faced challenges with connectivity issues. Despite this, customer service was prompt in helping me \\\n",
        "troubleshoot the problem.\"]\n",
        "\n",
        "# Run chain\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 temperature=0)\n",
        "\n",
        "chain = create_tagging_chain(schema, llm)"
      ],
      "metadata": {
        "id": "3KbZHHBnL_WH"
      },
      "id": "3KbZHHBnL_WH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in inp:\n",
        "  print(chain.invoke(i)[\"text\"])"
      ],
      "metadata": {
        "id": "7KxP_aUmWCQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f25fc2-e2d9-47a6-f6a1-ee8d2ddd1d38"
      },
      "id": "7KxP_aUmWCQN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentiment_for_product': 'positive', 'sentiment_for_customer_service_issue': 'negative', 'tecnical_problems': 'software issue', 'negative_customer_surves_experiences': 'frustrating experience with customer service'}\n",
            "{'sentiment_for_product': 'positive', 'sentiment_for_customer_service_issue': 'positive', 'tecnical_problems': \"difficulties with the phone's software updates\", 'negative_customer_surves_experiences': ''}\n",
            "{'sentiment_for_product': 'positive', 'sentiment_for_customer_service_issue': 'negative', 'tecnical_problems': 'issue with the speaker', 'negative_customer_surves_experiences': 'contact customer service multiple times'}\n",
            "{'sentiment_for_product': 'positive', 'sentiment_for_customer_service_issue': 'positive', 'tecnical_problems': 'lack of timely software updates', 'negative_customer_surves_experiences': ''}\n",
            "{'sentiment_for_product': 'positive', 'sentiment_for_customer_service_issue': 'positive', 'tecnical_problems': 'connectivity issues', 'negative_customer_surves_experiences': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. \"Telefonun pil ömrü tatmin edici olmasına rağmen, yazılım sorunuyla ilgili yardım gerektiğinde müşteri hizmetleriyle yaşadığım deneyim hayal kırıklığı yarattı.\"\n",
        "2. \"Kameranın düşük ışık performansı mükemmel, ancak telefonun yazılım güncellemeleriyle ilgili zorluklarla karşılaştım. Neyse ki müşteri hizmetleri ekibi sorunu çözmede yardımcı oldu.\"\n",
        "3. \"Telefonun tasarımı etkileyici, ancak hoparlörle ilgili bir sorunu çözmek için müşteri hizmetleriyle birçok kez iletişime geçmek zorunda kaldım.\"\n",
        "4. \"Genel olarak telefonun performansından memnunum, ancak zamanında yazılım güncellemelerinin eksikliği hayal kırıklığı yaratıyor. Müşteri hizmetleri, yardım istediğimde duyarlıydı.\"\n",
        "5. \"Telefonun şık tasarımı dikkatimi çekti, ancak bağlantı sorunlarıyla ilgili zorluklar yaşadım. Buna rağmen, müşteri hizmetleri sorunu çözmemde hızlı bir şekilde yardımcı oldu.\""
      ],
      "metadata": {
        "id": "8MC0uyRJmH8s"
      },
      "id": "8MC0uyRJmH8s"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.chains import create_tagging_chain_pydantic\n",
        "\n",
        "\n",
        "\n",
        "class Classification(BaseModel):\n",
        "    sentiment_for_product: str = Field(description=\"The sentiment for product\",\n",
        "                                       enum=[\"positive\", \"neutral\", \"negative\"])\n",
        "\n",
        "    sentiment_for_customer_service_issue: str = Field(description=\"The sentiment for customer service issues\",\n",
        "                                                      enum=[\"positive\", \"neutral\", \"negative\"])\n",
        "\n",
        "    tecnical_problems: str = Field(description=\"Details of the technical problem encountered with the products.\")\n",
        "\n",
        "    negative_customer_surves_experiences: str =Field(description=\"Details of the negative customer service experiences\")\n",
        "\n",
        "chain2 = create_tagging_chain_pydantic(Classification, llm)\n",
        "for i in inp:\n",
        "  print(chain2.invoke(i)[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiidCpadlhze",
        "outputId": "8295564f-0c85-4213-b765-ac2ac975b96a"
      },
      "id": "AiidCpadlhze",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment_for_product='positive' sentiment_for_customer_service_issue='negative' tecnical_problems='software issue' negative_customer_surves_experiences='frustrating experience with customer service'\n",
            "sentiment_for_product='positive' sentiment_for_customer_service_issue='positive' tecnical_problems=\"difficulties with the phone's software updates\" negative_customer_surves_experiences=''\n",
            "sentiment_for_product='positive' sentiment_for_customer_service_issue='negative' tecnical_problems='issue with the speaker' negative_customer_surves_experiences='contact customer service multiple times'\n",
            "sentiment_for_product='positive' sentiment_for_customer_service_issue='positive' tecnical_problems='lack of timely software updates' negative_customer_surves_experiences=''\n",
            "sentiment_for_product='positive' sentiment_for_customer_service_issue='positive' tecnical_problems='connectivity issues' negative_customer_surves_experiences=''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Extract the desired information from the following passage.\n",
        "\n",
        "Only extract the properties mentioned in the 'Classification' function.\n",
        "\n",
        "Passage:\n",
        "{input}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model=\"gpt-4o-mini\")\n",
        "\n",
        "tagging_chain = tagging_prompt | llm.with_structured_output(Classification)\n",
        "for i in inp:\n",
        "  print(tagging_chain.invoke({\"input\": i}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXjVWpw8mul0",
        "outputId": "e91511f0-c2ae-4b28-c639-79ecc7b8488c"
      },
      "id": "oXjVWpw8mul0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment_for_product='positive' sentiment_for_customer_service_issue='negative' tecnical_problems='software issue' negative_customer_surves_experiences='frustrating experience with customer service'\n",
            "sentiment_for_product='positive' sentiment_for_customer_service_issue='positive' tecnical_problems=\"difficulties with the phone's software updates\" negative_customer_surves_experiences=''\n",
            "sentiment_for_product='positive' sentiment_for_customer_service_issue='negative' tecnical_problems='issue with the speaker' negative_customer_surves_experiences='contact customer service multiple times'\n",
            "sentiment_for_product='positive' sentiment_for_customer_service_issue='positive' tecnical_problems='lack of timely software updates' negative_customer_surves_experiences=''\n",
            "sentiment_for_product='positive' sentiment_for_customer_service_issue='positive' tecnical_problems='connectivity issues' negative_customer_surves_experiences=''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, using the discharge summary information of the patients, let's determine whether they have diabetes and hypertension, and score their likelihood of being a patient on a scale of 0 to 5."
      ],
      "metadata": {
        "id": "fJoYyESHVZfU"
      },
      "id": "fJoYyESHVZfU"
    },
    {
      "cell_type": "code",
      "source": [
        "patient_1=\"\"\"Age: 50, Gender: Male,\n",
        "\n",
        "Symptoms:\n",
        "\n",
        "Experiencing persistent headaches lately.\n",
        "Feeling numbness in hands and feet, especially in the evenings.\n",
        "Frequently experiencing the need to urinate.\n",
        "\n",
        "Findings:\n",
        "\n",
        "Physical examination reveals retinal changes due to hypertension and signs of diabetic neuropathy.\n",
        "Blood pressure measurement indicates 160/100 mmHg.\n",
        "Blood tests reveal elevated blood sugar levels (fasting blood glucose of 200 mg/dL) and high cholesterol levels.\"\"\"\n",
        "\n",
        "patient_2=\"\"\"Age: 60, Gender: Female\n",
        "\n",
        "Symptoms:\n",
        "\n",
        "Experiencing blurred vision lately.\n",
        "Constantly feeling tired.\n",
        "Experiencing weakness and fatigue.\n",
        "Findings:\n",
        "\n",
        "Physical examination reveals retinopathy due to hypertension and signs of diabetic nephropathy.\n",
        "Blood pressure measurement indicates 170/110 mmHg.\n",
        "Blood tests reveal high blood sugar levels (HbA1c levels above 9%) and elevated triglyceride levels.\"\"\"\n",
        "\n",
        "patient_3=\"\"\"Age: 55, Gender: Male\n",
        "\n",
        "Symptoms:\n",
        "\n",
        "Experiencing frequent urination, especially during the night.\n",
        "Feeling constantly thirsty and drinking large amounts of water.\n",
        "Noticing unexplained weight loss despite normal or increased appetite.\n",
        "Findings:\n",
        "\n",
        "Physical examination reveals signs of peripheral neuropathy, such as tingling or numbness in the feet.\n",
        "Blood tests show elevated fasting blood glucose levels (above 126 mg/dL) and HbA1c levels indicating poorly controlled diabetes.\n",
        "Urinalysis indicates the presence of glucose in the urine, suggestive of uncontrolled diabetes.\"\"\"\n",
        "\n",
        "patient_4=\"\"\"Age: 35\n",
        "Gender: Female\n",
        "\n",
        "Symptoms:\n",
        "\n",
        "Experiencing frequent headaches, particularly in the morning.\n",
        "Feeling dizzy or lightheaded, especially when standing up quickly.\n",
        "Noticing chest pain or discomfort, especially during physical exertion.\n",
        "Findings:\n",
        "\n",
        "Blood pressure measurements consistently indicate elevated levels (systolic blood pressure above 140 mmHg and/or diastolic blood pressure above 90 mmHg).\n",
        "Fundoscopic examination may reveal signs of hypertensive retinopathy, such as retinal hemorrhages or cotton-wool spots.\n",
        "Laboratory tests may show elevated cholesterol or triglyceride levels, contributing to hypertension.\"\"\"\n",
        "\n",
        "patient_5=\"\"\"Age: 45\n",
        "Gender: Male\n",
        "\n",
        "Symptoms:\n",
        "\n",
        "Experiencing sudden onset of high fever, typically above 100.4°F (38°C).\n",
        "Complaining of body aches and muscle soreness all over the body.\n",
        "Having a persistent dry cough, sometimes accompanied by chest discomfort.\n",
        "Feeling fatigued and weak, often experiencing extreme tiredness.\n",
        "Findings:\n",
        "\n",
        "Physical examination may reveal redness and inflammation of the throat.\n",
        "Auscultation of the lungs may reveal crackles or wheezing due to inflammation.\n",
        "Rapid antigen tests or PCR tests may confirm the presence of influenza virus in respiratory secretions.\"\"\"\n",
        "\n",
        "patients= [patient_1, patient_2, patient_3, patient_4, patient_5]"
      ],
      "metadata": {
        "id": "RPMXBHOZM5oa"
      },
      "id": "RPMXBHOZM5oa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Schema\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"diabetes\":{\"type\": \"string\",\n",
        "                    \"enum\":[\"yes\", \"no\"],\n",
        "                    \"description\": \"Classify the patient as having diabetes.\"},\n",
        "         \"hypertension\":{\"type\": \"string\",\n",
        "                        \"enum\":[\"yes\", \"no\"],\n",
        "                        \"description\": \"Classify the patient as having hypertension.\"},\n",
        "        \"diabetes_likelihood\": {\"type\": \"number\",\n",
        "                                 \"description\": \"score of the patient having diabetes on a scale from 0 to 5.\",\n",
        "                                 \"minimum\":0,\n",
        "                                 \"maximum\":5},\n",
        "        \"hypertension_likelihood\": {\"type\": \"number\",\n",
        "                                    \"description\": \"score of the patient having hypertension on a scale from 0 to 5.\",\n",
        "                                    \"minimum\":0,\n",
        "                                    \"maximum\":5}\n",
        "    },\n",
        "    \"required\": [\"diabetes\", \"hypertension\", \"diabetes_likelihood\",  \"hypertension_likelihood\"]\n",
        "}"
      ],
      "metadata": {
        "id": "AiVgKHIlViVP"
      },
      "id": "AiVgKHIlViVP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run chain\n",
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model=\"gpt-4o-mini\")\n",
        "chain = create_tagging_chain(schema, llm)"
      ],
      "metadata": {
        "id": "2WdLbmqwVrZO"
      },
      "id": "2WdLbmqwVrZO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in patients:\n",
        "  print(chain.invoke(i)[\"text\"])"
      ],
      "metadata": {
        "id": "1OUWIG-OWJ3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d21e1cdb-5735-440b-dbbb-a92d9463bca6"
      },
      "id": "1OUWIG-OWJ3Y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'diabetes': 'yes', 'hypertension': 'yes', 'diabetes_likelihood': 5, 'hypertension_likelihood': 5}\n",
            "{'diabetes': 'yes', 'hypertension': 'yes', 'diabetes_likelihood': 5, 'hypertension_likelihood': 5}\n",
            "{'diabetes': 'yes', 'hypertension': 'no', 'diabetes_likelihood': 5, 'hypertension_likelihood': 0}\n",
            "{'diabetes': 'no', 'hypertension': 'yes', 'diabetes_likelihood': 0, 'hypertension_likelihood': 5}\n",
            "{'diabetes': 'no', 'hypertension': 'no', 'diabetes_likelihood': 0, 'hypertension_likelihood': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classification(BaseModel):\n",
        "    diabetes: str = Field(description=\"Classify the patient as having diabetes\",\n",
        "                          enum=[\"yes\", \"no\"])\n",
        "    hypertension: str = Field(description=\"Classify the patient as having hypertension\",\n",
        "                              enum=[\"yes\", \"no\"])\n",
        "    diabetes_likelihood: float = Field(description=\"score of the patient having diabetes on a scale from 0 to 5.\",\n",
        "                                       ge=0, le=5,) #ge=min, le=max\n",
        "    hypertension_likelihood: float =Field(description=\"score of the patient having hypertension on a scale from 0 to 5\",\n",
        "                                          ge=0, le=5)\n",
        "\n",
        "chain2 = create_tagging_chain_pydantic(Classification, llm)\n",
        "for i in patients:\n",
        "  print(chain2.invoke(i)[\"text\"])"
      ],
      "metadata": {
        "id": "CbYpN8W7WcfY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc9ae3f-eb06-4400-bee1-688d23aa49a6"
      },
      "id": "CbYpN8W7WcfY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diabetes='yes' hypertension='yes' diabetes_likelihood=5.0 hypertension_likelihood=5.0\n",
            "diabetes='yes' hypertension='yes' diabetes_likelihood=5.0 hypertension_likelihood=5.0\n",
            "diabetes='yes' hypertension='no' diabetes_likelihood=5.0 hypertension_likelihood=0.0\n",
            "diabetes='no' hypertension='yes' diabetes_likelihood=0.0 hypertension_likelihood=5.0\n",
            "diabetes='no' hypertension='no' diabetes_likelihood=0.0 hypertension_likelihood=0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Extract the desired information from the following passage.\n",
        "\n",
        "Only extract the properties mentioned in the 'Classification' function.\n",
        "\n",
        "Passage:\n",
        "{input}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model=\"gpt-4o-mini\")\n",
        "\n",
        "tagging_chain = tagging_prompt | llm.with_structured_output(Classification)\n",
        "for i in patients:\n",
        "  print(tagging_chain.invoke({\"input\": i}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcWBKMVhs_Tf",
        "outputId": "838ef76c-1b09-45f9-ba37-b52302709009"
      },
      "id": "IcWBKMVhs_Tf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diabetes='yes' hypertension='yes' diabetes_likelihood=5.0 hypertension_likelihood=5.0\n",
            "diabetes='yes' hypertension='yes' diabetes_likelihood=5.0 hypertension_likelihood=5.0\n",
            "diabetes='yes' hypertension='no' diabetes_likelihood=5.0 hypertension_likelihood=0.0\n",
            "diabetes='no' hypertension='yes' diabetes_likelihood=0.0 hypertension_likelihood=5.0\n",
            "diabetes='no' hypertension='no' diabetes_likelihood=0.0 hypertension_likelihood=0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Prompt"
      ],
      "metadata": {
        "id": "2Qlz4GPDWdAo"
      },
      "id": "2Qlz4GPDWdAo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Schema\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"sentiment_for_product\":{\"type\": \"string\",\n",
        "                                 \"enum\":[\"positive\", \"neutral\", \"negative\"],\n",
        "                                 \"description\": \"The sentiment for product\"},\n",
        "\n",
        "        \"tecnical_problems\": {\"type\": \"string\",\n",
        "                              \"description\": \"Details of the technical problem encountered with the products.\"},\n",
        "    },\n",
        "    \"required\": [\"sentiment_for_product\", \"tecnical_problems\"]\n",
        "}\n",
        "\n",
        "# Input\n",
        "inp = [\"Although the phone's battery life is satisfactory, I had a frustrating experience with customer service when I needed help with a software issue.\",\n",
        "\"The camera's low-light performance is excellent, but I encountered difficulties with the phone's software updates. Fortunately, the customer service team was \\\n",
        "helpful in resolving the issue.\",\n",
        "\"The design of the phone is impressive, but I had to contact customer service multiple times to address an issue with the speaker.\",\n",
        "\"I'm satisfied with the phone's performance overall, but the lack of timely software updates is disappointing. Customer service was responsive when \\\n",
        "I reached out for assistance. \",\n",
        "\"The phone's sleek design caught my eye, but I faced challenges with connectivity issues. Despite this, customer service was prompt in helping me \\\n",
        "troubleshoot the problem.\"]\n"
      ],
      "metadata": {
        "id": "uNpIgOtifdVc"
      },
      "id": "uNpIgOtifdVc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model=\"gpt-4o-mini\")\n",
        "chain = create_tagging_chain(schema, llm)"
      ],
      "metadata": {
        "id": "ssMUVUqpWqqA"
      },
      "id": "ssMUVUqpWqqA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#default chain prompt\n",
        "print(chain.prompt.messages[0].prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9o0QYxko3Pd",
        "outputId": "59ef2ef6-ec11-4a7f-b4e2-c8a818500148"
      },
      "id": "z9o0QYxko3Pd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extract the desired information from the following passage.\n",
            "\n",
            "Only extract the properties mentioned in the 'information_extraction' function.\n",
            "\n",
            "Passage:\n",
            "{input}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjzVRvfS7oAk",
        "outputId": "6f31dda2-3a08-4aec-926f-a4184570655e"
      },
      "id": "tjzVRvfS7oAk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template=\"Extract the desired information from the following passage.\\n\\nOnly extract the properties mentioned in the 'information_extraction' function.\\n\\nPassage:\\n{input}\\n\"))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7c681e1bb070>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7c681e29a980>, root_client=<openai.OpenAI object at 0x7c681e16ad70>, root_async_client=<openai.AsyncOpenAI object at 0x7c681e1bb0a0>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), output_parser=JsonOutputFunctionsParser(), llm_kwargs={'functions': [{'name': 'information_extraction', 'description': 'Extracts the relevant information from the passage.', 'parameters': {'type': 'object', 'properties': {'sentiment_for_product': {'title': 'sentiment_for_product', 'type': 'string', 'enum': ['positive', 'neutral', 'negative'], 'description': 'The sentiment for product'}, 'tecnical_problems': {'title': 'tecnical_problems', 'type': 'string', 'description': 'Details of the technical problem encountered with the products.'}}, 'required': ['sentiment_for_product', 'tecnical_problems']}}], 'function_call': {'name': 'information_extraction'}})"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yua2RhEoLAW",
        "outputId": "30f35e4d-7628-41d8-efa9-ef1dcf63aad2"
      },
      "id": "8yua2RhEoLAW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template=\"Extract the desired information from the following passage.\\n\\nOnly extract the properties mentioned in the 'information_extraction' function.\\n\\nPassage:\\n{input}\\n\"))]) llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7c681e1bb070>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7c681e29a980>, root_client=<openai.OpenAI object at 0x7c681e16ad70>, root_async_client=<openai.AsyncOpenAI object at 0x7c681e1bb0a0>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='') output_parser=JsonOutputFunctionsParser() llm_kwargs={'functions': [{'name': 'information_extraction', 'description': 'Extracts the relevant information from the passage.', 'parameters': {'type': 'object', 'properties': {'sentiment_for_product': {'title': 'sentiment_for_product', 'type': 'string', 'enum': ['positive', 'neutral', 'negative'], 'description': 'The sentiment for product'}, 'tecnical_problems': {'title': 'tecnical_problems', 'type': 'string', 'description': 'Details of the technical problem encountered with the products.'}}, 'required': ['sentiment_for_product', 'tecnical_problems']}}], 'function_call': {'name': 'information_extraction'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template_prompt = \"\"\"Extract the desired information from the following passage.\n",
        "\n",
        "Only extract the properties mentioned in the 'information_extraction' function.\n",
        "\n",
        "capitalize the first letter of the tecnical_problems properties.\n",
        "\n",
        "\n",
        "Passage:\n",
        "{input}\n",
        "\"\"\"\n",
        "\n",
        "chain.prompt.messages[0].prompt.template=template_prompt"
      ],
      "metadata": {
        "id": "Bhqu2VXKpGK2"
      },
      "id": "Bhqu2VXKpGK2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv349ets8Ya8",
        "outputId": "d7efc1f4-bf72-4a76-88c8-7751046244f1"
      },
      "id": "mv349ets8Ya8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template=\"Extract the desired information from the following passage.\\n\\nOnly extract the properties mentioned in the 'information_extraction' function.\\n\\ncapitalize the first letter of the tecnical_problems properties.\\n\\n\\nPassage:\\n{input}\\n\"))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7c681e1bb070>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7c681e29a980>, root_client=<openai.OpenAI object at 0x7c681e16ad70>, root_async_client=<openai.AsyncOpenAI object at 0x7c681e1bb0a0>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), output_parser=JsonOutputFunctionsParser(), llm_kwargs={'functions': [{'name': 'information_extraction', 'description': 'Extracts the relevant information from the passage.', 'parameters': {'type': 'object', 'properties': {'sentiment_for_product': {'title': 'sentiment_for_product', 'type': 'string', 'enum': ['positive', 'neutral', 'negative'], 'description': 'The sentiment for product'}, 'tecnical_problems': {'title': 'tecnical_problems', 'type': 'string', 'description': 'Details of the technical problem encountered with the products.'}}, 'required': ['sentiment_for_product', 'tecnical_problems']}}], 'function_call': {'name': 'information_extraction'}})"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in inp:\n",
        "  print(chain.invoke(i)[\"text\"])"
      ],
      "metadata": {
        "id": "_kwj8wwRXQgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebca0d0-0e2e-43b0-b2a2-c8556efa4557"
      },
      "id": "_kwj8wwRXQgZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentiment_for_product': 'neutral', 'tecnical_problems': 'Software Issue'}\n",
            "{'sentiment_for_product': 'positive', 'tecnical_problems': \"Difficulties with the phone's software updates.\"}\n",
            "{'sentiment_for_product': 'neutral', 'tecnical_problems': 'Issue With The Speaker'}\n",
            "{'sentiment_for_product': 'positive', 'tecnical_problems': 'Lack of timely software updates'}\n",
            "{'sentiment_for_product': 'neutral', 'tecnical_problems': 'Connectivity issues.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Sentiment(BaseModel):\n",
        "    sentiment_for_product: str = Field(description=\"The sentiment for product\",\n",
        "                                       enum=[\"positive\", \"neutral\", \"negative\"])\n",
        "\n",
        "    tecnical_problems: str = Field(description=\"Details of the technical problem encountered with the product.\")"
      ],
      "metadata": {
        "id": "dRdS5RA7CAZo"
      },
      "id": "dRdS5RA7CAZo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 temperature=0)\n",
        "#chain = tagging_prompt | llm.with_structured_output(schema=Person)\n",
        "chain = create_tagging_chain_pydantic(Sentiment, llm)\n",
        "for i in inp:\n",
        "  print(chain.invoke(i)[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_94G0JqRBXD7",
        "outputId": "27ec64f9-6483-4ad1-ed1f-ed3f065904eb"
      },
      "id": "_94G0JqRBXD7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment_for_product='neutral' tecnical_problems='software issue'\n",
            "sentiment_for_product='positive' tecnical_problems=\"difficulties with the phone's software updates\"\n",
            "sentiment_for_product='neutral' tecnical_problems='issue with the speaker'\n",
            "sentiment_for_product='positive' tecnical_problems='lack of timely software updates'\n",
            "sentiment_for_product='neutral' tecnical_problems='connectivity issues'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUogSrGpGMky",
        "outputId": "3fcc95ca-ca6c-4c35-f2c4-8b3f7b9cf8a8"
      },
      "id": "KUogSrGpGMky",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Although the phone's battery life is satisfactory, I had a frustrating experience with customer service when I needed help with a software issue.\",\n",
              " \"The camera's low-light performance is excellent, but I encountered difficulties with the phone's software updates. Fortunately, the customer service team was helpful in resolving the issue.\",\n",
              " 'The design of the phone is impressive, but I had to contact customer service multiple times to address an issue with the speaker.',\n",
              " \"I'm satisfied with the phone's performance overall, but the lack of timely software updates is disappointing. Customer service was responsive when I reached out for assistance. \",\n",
              " \"The phone's sleek design caught my eye, but I faced challenges with connectivity issues. Despite this, customer service was prompt in helping me troubleshoot the problem.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dWCU5yDBugUt"
      },
      "id": "dWCU5yDBugUt",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}